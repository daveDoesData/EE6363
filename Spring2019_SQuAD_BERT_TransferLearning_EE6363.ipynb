{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Spring2019-SQuAD-BERT-TransferLearning-EE6363.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/daveDoesData/EE6363/blob/master/Spring2019_SQuAD_BERT_TransferLearning_EE6363.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RHgPZ3mxHEqa",
        "colab_type": "text"
      },
      "source": [
        "**David Hardage, EE 6363 Spring 2019 Final Project**\n",
        "\n",
        "#Introduction\n",
        "To quote numerous bloggers, writers and natural language processing (NLP) enthusiast, last year marked NLP’s “ImageNet moment”. In 2018, the introduction of large pretrained models enabled practitioners to utilize transfer learning in language modeling task. The three most well known of these models ELMo, GPT, and BERT set the bar on performance for many NLP benchmark tasks. Of these three, BERT (Bidirectional Encoder Representations from Transformers) ended the year as “king” of the NLP hill. In this write up, I will cover the inner workings of BERT and look at BERT’s use in a question answering task using the Stanford Question Answering Dataset (SQuAD) 2.0.  \n",
        "\n",
        "To start, let’s briefly look at BERT’s preticessors the Allen Institute’s ELMo and  OpenAI’s GPT.\n",
        "\n",
        "![alt text](https://1.bp.blogspot.com/-RLAbr6kPNUo/W9is5FwUXmI/AAAAAAAADeU/5y9466Zoyoc96vqLjbruLK8i_t8qEdHnQCLcBGAs/s640/image3.png)\n",
        "\n",
        "Image: Google AI Blog [1]\n",
        "\n",
        "ELMo’s architecture is based on bidirectional LSTMs, so this language model has a sense of the preceding and following word. In addition, ELMo’s embeddings are based on the entire sequence, so the term “bank” will have a different representation in the sentences “I am going to the bank” and “You fell off the river bank”[2]. While ELMo is usable pretrained, it requires incorporation into more complex task specific architectures for state of the art performance. Building off of ELMo,  OpenAI’s GPT uses transformers which allow for the pretrained model to be used for state of the are performance with less architectural overhead [4]. However, GPT is not bidirectional like ELMo. In a way, BERT incorporates the best of ELMo and GPT into its architecture. Like GPT, BERT’s architecture consist of transformers. However BERT incorporates bidirectionality. Before diving much further into BERT, I will provide a quick recap of attention and describe the transformers mentioned in the paragraph. \n",
        "\n",
        "# Transformers and Attention \n",
        "\n",
        "No, it is not a Decepticon or Autobot. A transformer is a new neural network architecture which uses attention without any recurrent cells. The absence of recurrence increases the ability of practitioners to distribute computation when using transformer based architectures [5]. In order to better understand how a transformer functions, I will provide a short recap of attention in sequence to sequence networks. \n",
        "\n",
        "![alt text](https://drive.google.com/uc?export=view&id=1fnj4Ue19_bSfD92fdzdGFAieZ5e5YkvK)\n",
        "\n",
        "Image: NUS CS6101 Deep Learning for NLP S8 [6]\n",
        "\n",
        "In the diagram above, the encoder has hidden states (in red) for each time step in the input sequence. These hidden states contain some information for that particular token at that time step. The dot product of the decoder RNN hidden state (in green) and each hidden state in the encoder RNN creates the attention scores. These attention scores go through a softmax to produce the attention distribution. The resulting attention distribution is combined with the encoder’s hidden states to product a weighted sum vector with the dimensions of your hidden states. This attention vector contains more information from the encoder hidden states which received the most “attention” [6].\n",
        "\n",
        "Transformers operate off of three vector types queries (Q), keys (K) and values (V). In context to the diagram above, the keys are the encoder’s hidden state and the query vectors the decoder hidden state. The dot product and softmax of these two is combined with the values vector to obtain the attention output. In a full transformer architecture, this process takes place in encoder and decoder cells. In the diagram below, the encoder is represented on the left and the decoder on the right. \n",
        "\n",
        " ![alt text](https://drive.google.com/uc?export=view&id=1nTMbuHOEfdH7pabqXwai_7yVK8WR03GV)\n",
        "Image: All You Need is Attention [5]\n",
        "\n",
        "Since there are not any recurrent cells in this architecture, all inputs are summed with a positional encoding (PE) based on the token’s location in a sequence. In the paper, this PE is generated with sin and cosine. The authors hypothesize this will allow the model to better handle inputs larger than those seen in training. BERT does not use this type of PE; instead it uses a simple token embedding for positional information.\n",
        "\n",
        "Once the PE is added to  the input, Q,K, and V move into the multi-head attention portion (black circle). First, these vectors are pass through a linear projection. The result is multiple weighted Q,K, and V vectors all randomly initialized. Since their initializations are not the same, each of these representation sub spaces will look at the relationships between the input tokens in each sequence differently. \n",
        "\n",
        "Now that we have Q, K and V in multiple representation subspaces, each moves into scaled dot product attention (purple circle). The attention is the same as described in the above recap of attention, so let’s focus on the one difference: the scale. The dot product of Q and V is scaled by the square root of the dimension of the key vector. This helps to ensure a more stable gradient during training. \n",
        " \n",
        " ![alt text](https://cdn-images-1.medium.com/max/1800/1*lH5NKkkZjsGvjQmlis3_uw.png)\n",
        " \n",
        "Image: All You Need is Attention [5]\n",
        "\n",
        "To put all of the attention spaces back together, each head is concatenated to each other and multiplied by a weight matrix of the output which is trained jointly with the model. In short, this step takes all the outputs from the different attention heads combines their information and puts them into the correct dimensions to move forward into the model.\n",
        "\n",
        "The next step in the encoder cell is a residual connection where an element wise addition with the original input and normalization of this sum is performed. In essence, this just adds learned information to the original input and normalizes to keep it from growing too big. After this residual connection, the tokens are passed into a feed forward network. This network is considered position wise because each vector passes through the exact same network separately. Overall, this network acts a convolutional layer with a kernel size of one. Last, the output of the feed forward network goes through another residual connection before leaving this particular encoding cell. \n",
        "\n",
        "As for the decoder, it essentially goes through the same process, but with the input shifted and masking. The masking is done to prevent the decoder from “cheating” by being able to “see” the correct token. Since BERT does not utilize decoders, I will stop here, but I have included some helpful visual representations of the decoder below:\n",
        "\n",
        "![alt text](https://i1.wp.com/mlexplained.com/wp-content/uploads/2019/02/Screen-Shot-2019-02-10-at-5.58.36-PM.png?resize=1024%2C539)\n",
        "\n",
        "Image: Paper Dissected: “Attention is All You Need” Explained [7]\n",
        "\n",
        "![alt text](https://i0.wp.com/mlexplained.com/wp-content/uploads/2019/02/Screen-Shot-2019-02-10-at-6.05.02-PM.png?resize=1024%2C470)\n",
        "Image: Paper Dissected: “Attention is All You Need” Explained [7]\n",
        "\n",
        "# BERT\n",
        "\n",
        "As mentioned above, BERT only uses the encoder cells from the transformer architecture. These encoders are stacked on top of each other and learn the relationships between words and sentences from two “pre-training” task:\n",
        "*   \"Masked Language Model\": 15% of tokens from an input sequence are masked (some are randomly replaced with the wrong word). The model then needs to predict the id of this masked token. \n",
        "*   \"Next Sentence Prediction: Takes input sentence pairs, replaces 50% of the second sentences with a random sentence, and trains to learn sentence relationships. Required for task like question answering.\n",
        "\n",
        "These task are performed with using word pieces from BookCorpus (800M words) and english Wikipedia (2,500M words) in two architectures:\n",
        "*   BERT-Base: 12-layer, 768-hidden, 12-heads, 110M parameters\n",
        "*   BERT-Large: 24-layer, 1024-hidden, 16-heads, 340M parameters\n",
        "\n",
        "There are cased and uncased versions of both Base and Large available for use depending on whether case is important on the language modeling task the pretrained network is being used to solve. For my implementation I use BERT-Base because BERT-Large requires too much memory using the standard implementation with the Adam optimizer on a GPU with 12-15gb of memory. To use BERT-Large, a cloud TPU is recommended [8]. \n",
        "\n",
        "Let's review the architecture used and how it functions in question answering task.\n",
        " ![alt text](https://drive.google.com/uc?export=view&id=1BW6o4k6ZCWAGierc6DPcZqAhtBdzyhWb)\n",
        " \n",
        "Starting from the bottom, I input questions followed by context paragraphs from SQuAD 2.0. This dataset contains 100,000 question context paragraph pairs in which the context contains the answer and 50,000 question context pairs where the question is unanswerable with the provided context. The intent of SQuAD 2.0 is to create more robust reading comprehension systems by challenging researchers to create systems which known when they cannot answer a question [9].\n",
        "\n",
        "For the input sequence pairs, the first sequence starts with [CLS] and the two sequences end with [SEP]. The words are tokenized by word piece and assigned three embeddings which are summed  up and passed forward into the model:\n",
        "*    Token Embedding: a 786 dimensional vector representation for each word piece.\n",
        "*    Sentence Embedding: a token to distinguish between sequences in a paired input. The first sequence received 0 as an input and the next receives 1.\n",
        "*    Positional Embedding: a learned positional embedding which supports sequence lengths up to the max input size of 512 tokens.  \n",
        "\n",
        "Once summed together these pass up into the first encoder cell which goes through the process explained earlier in the transformer section with one alteration. In BERT the feed forward network uses GELU instead of ReLU. GELU is used because of the performance improvements it has over RELU due to its increased curvature (it can be negative) and non-monotonicity [10].\n",
        "\n",
        " ![alt text](https://drive.google.com/uc?export=view&id=1Fs3onc3l4exOYHbYK-CKrYaHi8Jc87Ss)\n",
        " \n",
        "After the sequences pass through BERT, they hit a linear layer whose output is the position for the first and last token in the answer span. These predictions are refined during training using cross entropy loss.\n",
        "\n",
        "`\n",
        "loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "start_loss = loss_fct(start_logits, start_positions)\n",
        "end_loss = loss_fct(end_logits, end_positions)\n",
        "total_loss = (start_loss + end_loss) / 2\n",
        "`\n",
        "\n",
        "In order to account for null answers, a threshold is set (default of 0). During training the start and end logits are stored and if these start and end logits are stored. If the sum of these logits subtracted by the minimum the sum of all start and end logits does not surpass the threshold, then the model predicts “” (the response for no answer). \n",
        "\n",
        "I trained this model for approximately 4 hrs on one Tesla P100 using a batch size of 15 (reduced form the default 32 due to memory constraints), and I was able to achieve fairly impressive performance for detecting when the model was unable to answer a question:\n",
        "*    True Negative (No Answer) - 0.811\n",
        "*    False Negative - 0.231\n",
        "*    False Omission Rate - 0.222\n",
        "\n",
        "To conclude, BERT is a powerful tool for transfer learning. Even with a decreased batch size, the model  was able to correctly identify 81% of the questions it could not answer.  In the future, I plan to utilize multiple-GPUs for larger batch sizes and sequence lengths to fine tune questions answers. \n",
        "\n",
        "For this project, I used Hugging Face’s pytorch implementation of BERT. This implementation breaks down the different BERT components into easy to follow torch nn modules. This code is included in the appendix[11]. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qBb6GAVsQxIL",
        "colab_type": "text"
      },
      "source": [
        "# Sources \n",
        "[1]https://ai.googleblog.com/2018/11/open-sourcing-bert-state-of-art-pre.html\n",
        "\n",
        "[2]https://jalammar.github.io/illustrated-bert/\n",
        "\n",
        "[3]https://allennlp.org/elmo\n",
        "\n",
        "[4]https://openai.com/blog/language-unsupervised/\n",
        "\n",
        "[5] Attention is All You Need https://arxiv.org/abs/1706.03762\n",
        "\n",
        "[6] NUS CS6101 Deep Learning for NLP S8 https://www.youtube.com/watch?v=yCdl2afW88k\n",
        "\n",
        "[7]http://mlexplained.com/2017/12/29/attention-is-all-you-need-explained/\n",
        "\n",
        "[8]https://github.com/google-research/bert\n",
        "\n",
        "[9]https://rajpurkar.github.io/SQuAD-explorer/\n",
        "\n",
        "[10] https://arxiv.org/pdf/1606.08415.pdf\n",
        "\n",
        "[11]https://github.com/huggingface/pytorch-pretrained-BERT\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JXLSdJbS3fa5",
        "colab_type": "text"
      },
      "source": [
        "# Archive Bert Pytorch Implementation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "luPs24JmC8OB",
        "colab_type": "text"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1rb1AYLCfZCN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%capture\n",
        "pip install pytorch-pretrained-bert"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NLk06HC-CdeM",
        "colab_type": "code",
        "outputId": "cba87bde-760a-47f8-8a41-3318153a2b0f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "%%bash\n",
        "git clone https://github.com/huggingface/pytorch-pretrained-BERT.git"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'pytorch-pretrained-BERT'...\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Oz6gFKv2CspQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%%bash\n",
        "cp pytorch-pretrained-BERT/examples/run_squad.py ."
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2Mznyl-Nh0rm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "import of run_squad will fail due to issue calling args from file_utils with \n",
        "version of pytorch-pretrained-bert from pip this cell deletes that line\n",
        "\n",
        "deleted line:\n",
        "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE, \n",
        "WEIGHTS_NAME, CONFIG_NAME\n",
        "\"\"\"\n",
        "with open(\"run_squad.py\", \"r\") as infile:\n",
        "    lines = infile.readlines()\n",
        "with open(\"run_squad.py\", \"w\") as outfile:\n",
        "    for pos, line in enumerate(lines):\n",
        "        if pos != 36:\n",
        "            outfile.write(line)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7OeDI4ho3em-",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import copy\n",
        "import json\n",
        "import logging\n",
        "import math\n",
        "import os\n",
        "import shutil\n",
        "import tarfile\n",
        "import tempfile\n",
        "import sys\n",
        "from io import open\n",
        "from torch import nn\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from pytorch_pretrained_bert import modeling\n",
        "from pytorch_pretrained_bert import BertTokenizer, BertModel\n",
        "from pytorch_pretrained_bert.modeling import load_tf_weights_in_bert, BertConfig\n",
        "\n",
        "from pytorch_pretrained_bert.file_utils import PYTORCH_PRETRAINED_BERT_CACHE \n",
        "from run_squad import *\n",
        "\n",
        "\n",
        "# OPTIONAL: if you want to have more information on what's happening, activate the logger as follows\n",
        "import logging\n",
        "logging.basicConfig(level=logging.INFO)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wqiGwDSpfsPs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# mannually define config name and weights name args due to issue with version of pytorch-pretrained-bert from pip\n",
        "CONFIG_NAME = \"config.json\"\n",
        "WEIGHTS_NAME = \"pytorch_model.bin\"\n",
        "PRETRAINED_MODEL_ARCHIVE_MAP = {\n",
        "    'bert-base-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-uncased.tar.gz\",\n",
        "    'bert-large-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-uncased.tar.gz\",\n",
        "    'bert-base-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz\",\n",
        "    'bert-large-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-large-cased.tar.gz\",\n",
        "    'bert-base-multilingual-uncased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-uncased.tar.gz\",\n",
        "    'bert-base-multilingual-cased': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-multilingual-cased.tar.gz\",\n",
        "    'bert-base-chinese': \"https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-chinese.tar.gz\",\n",
        "}\n",
        "BERT_CONFIG_NAME = 'bert_config.json'\n",
        "TF_WEIGHTS_NAME = 'model.ckpt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TyuObNc8KTHs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def gelu(x):\n",
        "    \"\"\"Implementation of the gelu activation function.\n",
        "        For information: OpenAI GPT's gelu is slightly different (and gives slightly different results):\n",
        "        0.5 * x * (1 + torch.tanh(math.sqrt(2 / math.pi) * (x + 0.044715 * torch.pow(x, 3))))\n",
        "        Also see https://arxiv.org/abs/1606.08415\n",
        "    \"\"\"\n",
        "    return x * 0.5 * (1.0 + torch.erf(x / math.sqrt(2.0)))\n",
        "\n",
        "\n",
        "def swish(x):\n",
        "    return x * torch.sigmoid(x)\n",
        "\n",
        "\n",
        "ACT2FN = {\"gelu\": gelu, \"relu\": torch.nn.functional.relu, \"swish\": swish}"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pejyynPdKTME",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertEmbeddings(nn.Module):\n",
        "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertEmbeddings, self).__init__()\n",
        "        self.word_embeddings = nn.Embedding(config.vocab_size, config.hidden_size, padding_idx=0)\n",
        "        self.position_embeddings = nn.Embedding(config.max_position_embeddings, config.hidden_size)\n",
        "        self.token_type_embeddings = nn.Embedding(config.type_vocab_size, config.hidden_size)\n",
        "\n",
        "        # self.LayerNorm is not snake-cased to stick with TensorFlow model variable name and be able to load\n",
        "        # any TensorFlow checkpoint file\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None):\n",
        "        seq_length = input_ids.size(1)\n",
        "        position_ids = torch.arange(seq_length, dtype=torch.long, device=input_ids.device)\n",
        "        position_ids = position_ids.unsqueeze(0).expand_as(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        words_embeddings = self.word_embeddings(input_ids)\n",
        "        position_embeddings = self.position_embeddings(position_ids)\n",
        "        token_type_embeddings = self.token_type_embeddings(token_type_ids)\n",
        "\n",
        "        embeddings = words_embeddings + position_embeddings + token_type_embeddings\n",
        "        embeddings = self.LayerNorm(embeddings)\n",
        "        embeddings = self.dropout(embeddings)\n",
        "        return embeddings"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yf-wVvDLKTOt",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertSelfAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfAttention, self).__init__()\n",
        "        if config.hidden_size % config.num_attention_heads != 0:\n",
        "            raise ValueError(\n",
        "                \"The hidden size (%d) is not a multiple of the number of attention \"\n",
        "                \"heads (%d)\" % (config.hidden_size, config.num_attention_heads))\n",
        "        self.num_attention_heads = config.num_attention_heads\n",
        "        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)\n",
        "        self.all_head_size = self.num_attention_heads * self.attention_head_size\n",
        "\n",
        "        self.query = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.key = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "        self.value = nn.Linear(config.hidden_size, self.all_head_size)\n",
        "\n",
        "        self.dropout = nn.Dropout(config.attention_probs_dropout_prob)\n",
        "\n",
        "    def transpose_for_scores(self, x):\n",
        "        new_x_shape = x.size()[:-1] + (self.num_attention_heads, self.attention_head_size)\n",
        "        x = x.view(*new_x_shape)\n",
        "        return x.permute(0, 2, 1, 3)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        mixed_query_layer = self.query(hidden_states)\n",
        "        mixed_key_layer = self.key(hidden_states)\n",
        "        mixed_value_layer = self.value(hidden_states)\n",
        "\n",
        "        query_layer = self.transpose_for_scores(mixed_query_layer)\n",
        "        key_layer = self.transpose_for_scores(mixed_key_layer)\n",
        "        value_layer = self.transpose_for_scores(mixed_value_layer)\n",
        "\n",
        "        # Take the dot product between \"query\" and \"key\" to get the raw attention scores.\n",
        "        attention_scores = torch.matmul(query_layer, key_layer.transpose(-1, -2))\n",
        "        attention_scores = attention_scores / math.sqrt(self.attention_head_size)\n",
        "        # Apply the attention mask is (precomputed for all layers in BertModel forward() function)\n",
        "        attention_scores = attention_scores + attention_mask\n",
        "\n",
        "        # Normalize the attention scores to probabilities.\n",
        "        attention_probs = nn.Softmax(dim=-1)(attention_scores)\n",
        "\n",
        "        # This is actually dropping out entire tokens to attend to, which might\n",
        "        # seem a bit unusual, but is taken from the original Transformer paper.\n",
        "        attention_probs = self.dropout(attention_probs)\n",
        "\n",
        "        context_layer = torch.matmul(attention_probs, value_layer)\n",
        "        context_layer = context_layer.permute(0, 2, 1, 3).contiguous()\n",
        "        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)\n",
        "        context_layer = context_layer.view(*new_context_layer_shape)\n",
        "        return context_layer\n",
        "\n",
        "class BertSelfOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertSelfOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states\n",
        "\n",
        "class BertAttention(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertAttention, self).__init__()\n",
        "        self.self = BertSelfAttention(config)\n",
        "        self.output = BertSelfOutput(config)\n",
        "\n",
        "    def forward(self, input_tensor, attention_mask):\n",
        "        self_output = self.self(input_tensor, attention_mask)\n",
        "        attention_output = self.output(self_output, input_tensor)\n",
        "        return attention_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YYB5mKXQKTSB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertIntermediate(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertIntermediate, self).__init__()\n",
        "        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)\n",
        "        if isinstance(config.hidden_act, str) or (sys.version_info[0] == 2 and isinstance(config.hidden_act, unicode)):\n",
        "            self.intermediate_act_fn = ACT2FN[config.hidden_act]\n",
        "        else:\n",
        "            self.intermediate_act_fn = config.hidden_act\n",
        "\n",
        "    def forward(self, hidden_states):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.intermediate_act_fn(hidden_states)\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8rn77ysUKTVL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertOutput(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertOutput, self).__init__()\n",
        "        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)\n",
        "        self.LayerNorm = BertLayerNorm(config.hidden_size, eps=1e-12)\n",
        "        self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "\n",
        "    def forward(self, hidden_states, input_tensor):\n",
        "        hidden_states = self.dense(hidden_states)\n",
        "        hidden_states = self.dropout(hidden_states)\n",
        "        hidden_states = self.LayerNorm(hidden_states + input_tensor)\n",
        "        return hidden_states"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cu8v9PaHKTXp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertLayer(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertLayer, self).__init__()\n",
        "        self.attention = BertAttention(config)\n",
        "        self.intermediate = BertIntermediate(config)\n",
        "        self.output = BertOutput(config)\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask):\n",
        "        attention_output = self.attention(hidden_states, attention_mask)\n",
        "        intermediate_output = self.intermediate(attention_output)\n",
        "        layer_output = self.output(intermediate_output, attention_output)\n",
        "        return layer_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mk0XZR9PQcq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertPreTrainedModel(nn.Module):\n",
        "    \"\"\" An abstract class to handle weights initialization and\n",
        "        a simple interface for dowloading and loading pretrained models.\n",
        "    \"\"\"\n",
        "    def __init__(self, config, *inputs, **kwargs):\n",
        "        super(BertPreTrainedModel, self).__init__()\n",
        "        if not isinstance(config, BertConfig):\n",
        "            raise ValueError(\n",
        "                \"Parameter config in `{}(config)` should be an instance of class `BertConfig`. \"\n",
        "                \"To create a model from a Google pretrained model use \"\n",
        "                \"`model = {}.from_pretrained(PRETRAINED_MODEL_NAME)`\".format(\n",
        "                    self.__class__.__name__, self.__class__.__name__\n",
        "                ))\n",
        "        self.config = config\n",
        "\n",
        "    def init_bert_weights(self, module):\n",
        "        \"\"\" Initialize the weights.\n",
        "        \"\"\"\n",
        "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
        "            # Slightly different from the TF version which uses truncated_normal for initialization\n",
        "            # cf https://github.com/pytorch/pytorch/pull/5617\n",
        "            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)\n",
        "        elif isinstance(module, BertLayerNorm):\n",
        "            module.bias.data.zero_()\n",
        "            module.weight.data.fill_(1.0)\n",
        "        if isinstance(module, nn.Linear) and module.bias is not None:\n",
        "            module.bias.data.zero_()\n",
        "\n",
        "    @classmethod\n",
        "    def from_pretrained(cls, pretrained_model_name_or_path, state_dict=None, cache_dir=None,\n",
        "                        from_tf=False, *inputs, **kwargs):\n",
        "        \"\"\"\n",
        "        Instantiate a BertPreTrainedModel from a pre-trained model file or a pytorch state dict.\n",
        "        Download and cache the pre-trained model file if needed.\n",
        "        Params:\n",
        "            pretrained_model_name_or_path: either:\n",
        "                - a str with the name of a pre-trained model to load selected in the list of:\n",
        "                    . `bert-base-uncased`\n",
        "                    . `bert-large-uncased`\n",
        "                    . `bert-base-cased`\n",
        "                    . `bert-large-cased`\n",
        "                    . `bert-base-multilingual-uncased`\n",
        "                    . `bert-base-multilingual-cased`\n",
        "                    . `bert-base-chinese`\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `pytorch_model.bin` a PyTorch dump of a BertForPreTraining instance\n",
        "                - a path or url to a pretrained model archive containing:\n",
        "                    . `bert_config.json` a configuration file for the model\n",
        "                    . `model.chkpt` a TensorFlow checkpoint\n",
        "            from_tf: should we load the weights from a locally saved TensorFlow checkpoint\n",
        "            cache_dir: an optional path to a folder in which the pre-trained models will be cached.\n",
        "            state_dict: an optional state dictionnary (collections.OrderedDict object) to use instead of Google pre-trained models\n",
        "            *inputs, **kwargs: additional input for the specific Bert class\n",
        "                (ex: num_labels for BertForSequenceClassification)\n",
        "        \"\"\"\n",
        "        if pretrained_model_name_or_path in PRETRAINED_MODEL_ARCHIVE_MAP:\n",
        "            archive_file = PRETRAINED_MODEL_ARCHIVE_MAP[pretrained_model_name_or_path]\n",
        "        else:\n",
        "            archive_file = pretrained_model_name_or_path\n",
        "        # redirect to the cache, if necessary\n",
        "        try:\n",
        "            resolved_archive_file = cached_path(archive_file, cache_dir=cache_dir)\n",
        "        except EnvironmentError:\n",
        "            logger.error(\n",
        "                \"Model name '{}' was not found in model name list ({}). \"\n",
        "                \"We assumed '{}' was a path or url but couldn't find any file \"\n",
        "                \"associated to this path or url.\".format(\n",
        "                    pretrained_model_name_or_path,\n",
        "                    ', '.join(PRETRAINED_MODEL_ARCHIVE_MAP.keys()),\n",
        "                    archive_file))\n",
        "            return None\n",
        "        if resolved_archive_file == archive_file:\n",
        "            logger.info(\"loading archive file {}\".format(archive_file))\n",
        "        else:\n",
        "            logger.info(\"loading archive file {} from cache at {}\".format(\n",
        "                archive_file, resolved_archive_file))\n",
        "        tempdir = None\n",
        "        if os.path.isdir(resolved_archive_file) or from_tf:\n",
        "            serialization_dir = resolved_archive_file\n",
        "        else:\n",
        "            # Extract archive to temp dir\n",
        "            tempdir = tempfile.mkdtemp()\n",
        "            logger.info(\"extracting archive file {} to temp dir {}\".format(\n",
        "                resolved_archive_file, tempdir))\n",
        "            with tarfile.open(resolved_archive_file, 'r:gz') as archive:\n",
        "                archive.extractall(tempdir)\n",
        "            serialization_dir = tempdir\n",
        "        # Load config\n",
        "        config_file = os.path.join(serialization_dir, CONFIG_NAME)\n",
        "        if not os.path.exists(config_file):\n",
        "            # Backward compatibility with old naming format\n",
        "            config_file = os.path.join(serialization_dir, BERT_CONFIG_NAME)\n",
        "        config = BertConfig.from_json_file(config_file)\n",
        "        logger.info(\"Model config {}\".format(config))\n",
        "        # Instantiate model.\n",
        "        model = cls(config, *inputs, **kwargs)\n",
        "        if state_dict is None and not from_tf:\n",
        "            weights_path = os.path.join(serialization_dir, WEIGHTS_NAME)\n",
        "            state_dict = torch.load(weights_path, map_location='cpu')\n",
        "        if tempdir:\n",
        "            # Clean up temp dir\n",
        "            shutil.rmtree(tempdir)\n",
        "        if from_tf:\n",
        "            # Directly load from a TensorFlow checkpoint\n",
        "            weights_path = os.path.join(serialization_dir, TF_WEIGHTS_NAME)\n",
        "            return load_tf_weights_in_bert(model, weights_path)\n",
        "        # Load from a PyTorch state_dict\n",
        "        old_keys = []\n",
        "        new_keys = []\n",
        "        for key in state_dict.keys():\n",
        "            new_key = None\n",
        "            if 'gamma' in key:\n",
        "                new_key = key.replace('gamma', 'weight')\n",
        "            if 'beta' in key:\n",
        "                new_key = key.replace('beta', 'bias')\n",
        "            if new_key:\n",
        "                old_keys.append(key)\n",
        "                new_keys.append(new_key)\n",
        "        for old_key, new_key in zip(old_keys, new_keys):\n",
        "            state_dict[new_key] = state_dict.pop(old_key)\n",
        "\n",
        "        missing_keys = []\n",
        "        unexpected_keys = []\n",
        "        error_msgs = []\n",
        "        # copy state_dict so _load_from_state_dict can modify it\n",
        "        metadata = getattr(state_dict, '_metadata', None)\n",
        "        state_dict = state_dict.copy()\n",
        "        if metadata is not None:\n",
        "            state_dict._metadata = metadata\n",
        "\n",
        "        def load(module, prefix=''):\n",
        "            local_metadata = {} if metadata is None else metadata.get(prefix[:-1], {})\n",
        "            module._load_from_state_dict(\n",
        "                state_dict, prefix, local_metadata, True, missing_keys, unexpected_keys, error_msgs)\n",
        "            for name, child in module._modules.items():\n",
        "                if child is not None:\n",
        "                    load(child, prefix + name + '.')\n",
        "        start_prefix = ''\n",
        "        if not hasattr(model, 'bert') and any(s.startswith('bert.') for s in state_dict.keys()):\n",
        "            start_prefix = 'bert.'\n",
        "        load(model, prefix=start_prefix)\n",
        "        if len(missing_keys) > 0:\n",
        "            logger.info(\"Weights of {} not initialized from pretrained model: {}\".format(\n",
        "                model.__class__.__name__, missing_keys))\n",
        "        if len(unexpected_keys) > 0:\n",
        "            logger.info(\"Weights from pretrained model not used in {}: {}\".format(\n",
        "                model.__class__.__name__, unexpected_keys))\n",
        "        if len(error_msgs) > 0:\n",
        "            raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n",
        "                               model.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n",
        "        return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AQberEnDKTbh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertEncoder(nn.Module):\n",
        "    def __init__(self, config):\n",
        "        super(BertEncoder, self).__init__()\n",
        "        layer = BertLayer(config)\n",
        "        self.layer = nn.ModuleList([copy.deepcopy(layer) for _ in range(config.num_hidden_layers)])\n",
        "\n",
        "    def forward(self, hidden_states, attention_mask, output_all_encoded_layers=True):\n",
        "        all_encoder_layers = []\n",
        "        for layer_module in self.layer:\n",
        "            hidden_states = layer_module(hidden_states, attention_mask)\n",
        "            if output_all_encoded_layers:\n",
        "                all_encoder_layers.append(hidden_states)\n",
        "        if not output_all_encoded_layers:\n",
        "            all_encoder_layers.append(hidden_states)\n",
        "        return all_encoder_layers"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bTP7SPgNKThv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertModel(BertPreTrainedModel):\n",
        "    \"\"\"BERT model (\"Bidirectional Embedding Representations from a Transformer\").\n",
        "    Params:\n",
        "        config: a BertConfig class instance with the configuration to build a new model\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `output_all_encoded_layers`: boolean which controls the content of the `encoded_layers` output as described below. Default: `True`.\n",
        "    Outputs: Tuple of (encoded_layers, pooled_output)\n",
        "        `encoded_layers`: controled by `output_all_encoded_layers` argument:\n",
        "            - `output_all_encoded_layers=True`: outputs a list of the full sequences of encoded-hidden-states at the end\n",
        "                of each attention block (i.e. 12 full sequences for BERT-base, 24 for BERT-large), each\n",
        "                encoded-hidden-state is a torch.FloatTensor of size [batch_size, sequence_length, hidden_size],\n",
        "            - `output_all_encoded_layers=False`: outputs only the full sequence of hidden-states corresponding\n",
        "                to the last attention block of shape [batch_size, sequence_length, hidden_size],\n",
        "        `pooled_output`: a torch.FloatTensor of size [batch_size, hidden_size] which is the output of a\n",
        "            classifier pretrained on top of the hidden state associated to the first character of the\n",
        "            input (`CLS`) to train on the Next-Sentence task (see BERT's paper).\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = modeling.BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    model = modeling.BertModel(config=config)\n",
        "    all_encoder_layers, pooled_output = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertModel, self).__init__(config)\n",
        "        self.embeddings = BertEmbeddings(config)\n",
        "        self.encoder = BertEncoder(config)\n",
        "        self.pooler = BertPooler(config)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, output_all_encoded_layers=True):\n",
        "        if attention_mask is None:\n",
        "            attention_mask = torch.ones_like(input_ids)\n",
        "        if token_type_ids is None:\n",
        "            token_type_ids = torch.zeros_like(input_ids)\n",
        "\n",
        "        # We create a 3D attention mask from a 2D tensor mask.\n",
        "        # Sizes are [batch_size, 1, 1, to_seq_length]\n",
        "        # So we can broadcast to [batch_size, num_heads, from_seq_length, to_seq_length]\n",
        "        # this attention mask is more simple than the triangular masking of causal attention\n",
        "        # used in OpenAI GPT, we just need to prepare the broadcast dimension here.\n",
        "        extended_attention_mask = attention_mask.unsqueeze(1).unsqueeze(2)\n",
        "\n",
        "        # Since attention_mask is 1.0 for positions we want to attend and 0.0 for\n",
        "        # masked positions, this operation will create a tensor which is 0.0 for\n",
        "        # positions we want to attend and -10000.0 for masked positions.\n",
        "        # Since we are adding it to the raw scores before the softmax, this is\n",
        "        # effectively the same as removing these entirely.\n",
        "        extended_attention_mask = extended_attention_mask.to(dtype=next(self.parameters()).dtype) # fp16 compatibility\n",
        "        extended_attention_mask = (1.0 - extended_attention_mask) * -10000.0\n",
        "\n",
        "        embedding_output = self.embeddings(input_ids, token_type_ids)\n",
        "        encoded_layers = self.encoder(embedding_output,\n",
        "                                      extended_attention_mask,\n",
        "                                      output_all_encoded_layers=output_all_encoded_layers)\n",
        "        sequence_output = encoded_layers[-1]\n",
        "        pooled_output = self.pooler(sequence_output)\n",
        "        if not output_all_encoded_layers:\n",
        "            encoded_layers = encoded_layers[-1]\n",
        "        return encoded_layers, pooled_output"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gZF8iQWVKTka",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class BertForQuestionAnswering(BertPreTrainedModel):\n",
        "    \"\"\"BERT model for Question Answering (span extraction).\n",
        "    This module is composed of the BERT model with a linear layer on top of\n",
        "    the sequence output that computes start_logits and end_logits\n",
        "    Params:\n",
        "        `config`: a BertConfig class instance with the configuration to build a new model.\n",
        "    Inputs:\n",
        "        `input_ids`: a torch.LongTensor of shape [batch_size, sequence_length]\n",
        "            with the word token indices in the vocabulary(see the tokens preprocessing logic in the scripts\n",
        "            `extract_features.py`, `run_classifier.py` and `run_squad.py`)\n",
        "        `token_type_ids`: an optional torch.LongTensor of shape [batch_size, sequence_length] with the token\n",
        "            types indices selected in [0, 1]. Type 0 corresponds to a `sentence A` and type 1 corresponds to\n",
        "            a `sentence B` token (see BERT paper for more details).\n",
        "        `attention_mask`: an optional torch.LongTensor of shape [batch_size, sequence_length] with indices\n",
        "            selected in [0, 1]. It's a mask to be used if the input sequence length is smaller than the max\n",
        "            input sequence length in the current batch. It's the mask that we typically use for attention when\n",
        "            a batch has varying length sentences.\n",
        "        `start_positions`: position of the first token for the labeled span: torch.LongTensor of shape [batch_size].\n",
        "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
        "            into account for computing the loss.\n",
        "        `end_positions`: position of the last token for the labeled span: torch.LongTensor of shape [batch_size].\n",
        "            Positions are clamped to the length of the sequence and position outside of the sequence are not taken\n",
        "            into account for computing the loss.\n",
        "    Outputs:\n",
        "        if `start_positions` and `end_positions` are not `None`:\n",
        "            Outputs the total_loss which is the sum of the CrossEntropy loss for the start and end token positions.\n",
        "        if `start_positions` or `end_positions` is `None`:\n",
        "            Outputs a tuple of start_logits, end_logits which are the logits respectively for the start and end\n",
        "            position tokens of shape [batch_size, sequence_length].\n",
        "    Example usage:\n",
        "    ```python\n",
        "    # Already been converted into WordPiece token ids\n",
        "    input_ids = torch.LongTensor([[31, 51, 99], [15, 5, 0]])\n",
        "    input_mask = torch.LongTensor([[1, 1, 1], [1, 1, 0]])\n",
        "    token_type_ids = torch.LongTensor([[0, 0, 1], [0, 1, 0]])\n",
        "    config = BertConfig(vocab_size_or_config_json_file=32000, hidden_size=768,\n",
        "        num_hidden_layers=12, num_attention_heads=12, intermediate_size=3072)\n",
        "    model = BertForQuestionAnswering(config)\n",
        "    start_logits, end_logits = model(input_ids, token_type_ids, input_mask)\n",
        "    ```\n",
        "    \"\"\"\n",
        "    def __init__(self, config):\n",
        "        super(BertForQuestionAnswering, self).__init__(config)\n",
        "        self.bert = BertModel(config)\n",
        "        # TODO check with Google if it's normal there is no dropout on the token classifier of SQuAD in the TF version\n",
        "        # self.dropout = nn.Dropout(config.hidden_dropout_prob)\n",
        "        self.qa_outputs = nn.Linear(config.hidden_size, 2)\n",
        "        self.apply(self.init_bert_weights)\n",
        "\n",
        "    def forward(self, input_ids, token_type_ids=None, attention_mask=None, start_positions=None, end_positions=None):\n",
        "        sequence_output, _ = self.bert(input_ids, token_type_ids, attention_mask, output_all_encoded_layers=False)\n",
        "        logits = self.qa_outputs(sequence_output)\n",
        "        start_logits, end_logits = logits.split(1, dim=-1)\n",
        "        start_logits = start_logits.squeeze(-1)\n",
        "        end_logits = end_logits.squeeze(-1)\n",
        "\n",
        "        if start_positions is not None and end_positions is not None:\n",
        "            # If we are on multi-GPU, split add a dimension\n",
        "            if len(start_positions.size()) > 1:\n",
        "                start_positions = start_positions.squeeze(-1)\n",
        "            if len(end_positions.size()) > 1:\n",
        "                end_positions = end_positions.squeeze(-1)\n",
        "            # sometimes the start/end positions are outside our model inputs, we ignore these terms\n",
        "            ignored_index = start_logits.size(1)\n",
        "            start_positions.clamp_(0, ignored_index)\n",
        "            end_positions.clamp_(0, ignored_index)\n",
        "\n",
        "            loss_fct = CrossEntropyLoss(ignore_index=ignored_index)\n",
        "            start_loss = loss_fct(start_logits, start_positions)\n",
        "            end_loss = loss_fct(end_logits, end_positions)\n",
        "            total_loss = (start_loss + end_loss) / 2\n",
        "            return total_loss\n",
        "        else:\n",
        "            return start_logits, end_logits\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqbG2SsHoF_K",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained(bert_model, cache_dir=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYI7WS4ZoGId",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qt5pZKHUKTm3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "torch.cuda.set_device(0)\n",
        "device = torch.device(\"cuda\", 0)\n",
        "n_gpu = 1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SufLS5oWKTpg",
        "colab_type": "code",
        "outputId": "2b27bc73-65cf-4d17-e46f-18f9ccb9cf37",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 145
        }
      },
      "source": [
        "train_file = 'train-v2.0.json'\n",
        "predict_file = 'dev-v2.0.json' \n",
        "output_dir = 'squad2_log'\n",
        "bert_model = 'bert-base-cased'\n",
        "train_batch_size = 32\n",
        "num_train_epochs = 3\n",
        "learning_rate = 5e-5\n",
        "gradient_accumulation_steps = 1 #Number of updates steps to accumulate before performing a backward/update pass.\n",
        "warmup_proportion = 0.1\n",
        "doc_stride = 128\n",
        "max_query_length = 64\n",
        "max_seq_length = 384\n",
        "\n",
        "predict_batch_size = 8\n",
        "n_best_size = 20\n",
        "max_answer_length = 30\n",
        "\n",
        "seed = 42\n",
        "\n",
        "tokenizer = BertTokenizer.from_pretrained(bert_model, do_lower_case=False)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt not found in cache, downloading to /tmp/tmpf6ohop58\n",
            "100%|██████████| 213450/213450 [00:00<00:00, 2465797.03B/s]\n",
            "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmpf6ohop58 to cache at /root/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n",
            "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmpf6ohop58\n",
            "INFO:pytorch_pretrained_bert.tokenization:loading vocabulary file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased-vocab.txt from cache at /root/.pytorch_pretrained_bert/5e8a2b4893d13790ed4150ca1906be5f7a03d6c4ddf62296c383f6db42814db2.e13dbb970cb325137104fb2e5f36fe865f27746c6b526f6352861b1980eb80b1\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ex9wOE5OKTsU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "if os.path.exists(output_dir) and os.listdir(output_dir):\n",
        "  raise ValueError(\"Output directory () already exists and is not empty.\")\n",
        "if not os.path.exists(output_dir):\n",
        "  os.makedirs(output_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHCqfMQ4nelG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3aomt18nerB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = BertForQuestionAnswering.from_pretrained(bert_model, cache_dir=None)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NNV24DiaKTvN",
        "colab_type": "code",
        "outputId": "dd21abc4-0dec-472a-fd94-66c963aa6771",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 5460
        }
      },
      "source": [
        "train_examples = read_squad_examples(input_file=train_file, is_training=True, version_2_with_negative=True)\n",
        "num_train_optimization_steps = int(len(train_examples)/train_batch_size / gradient_accumulation_steps) * num_train_epochs\n",
        "\n",
        "# Prepare model\n",
        "model = BertForQuestionAnswering.from_pretrained(bert_model, cache_dir=None)\n",
        "model.to(device)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:pytorch_pretrained_bert.file_utils:https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz not found in cache, downloading to /tmp/tmplmxrjei1\n",
            "100%|██████████| 404400730/404400730 [00:07<00:00, 55536672.01B/s]\n",
            "INFO:pytorch_pretrained_bert.file_utils:copying /tmp/tmplmxrjei1 to cache at /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
            "INFO:pytorch_pretrained_bert.file_utils:creating metadata file for /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
            "INFO:pytorch_pretrained_bert.file_utils:removing temp file /tmp/tmplmxrjei1\n",
            "INFO:pytorch_pretrained_bert.modeling:loading archive file https://s3.amazonaws.com/models.huggingface.co/bert/bert-base-cased.tar.gz from cache at /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c\n",
            "INFO:pytorch_pretrained_bert.modeling:extracting archive file /root/.pytorch_pretrained_bert/a803ce83ca27fecf74c355673c434e51c265fb8a3e0e57ac62a80e38ba98d384.681017f415dfb33ec8d0e04fe51a619f3f01532ecea04edbfd48c5d160550d9c to temp dir /tmp/tmpaj4ybdo4\n",
            "INFO:pytorch_pretrained_bert.modeling:Model config {\n",
            "  \"attention_probs_dropout_prob\": 0.1,\n",
            "  \"hidden_act\": \"gelu\",\n",
            "  \"hidden_dropout_prob\": 0.1,\n",
            "  \"hidden_size\": 768,\n",
            "  \"initializer_range\": 0.02,\n",
            "  \"intermediate_size\": 3072,\n",
            "  \"max_position_embeddings\": 512,\n",
            "  \"num_attention_heads\": 12,\n",
            "  \"num_hidden_layers\": 12,\n",
            "  \"type_vocab_size\": 2,\n",
            "  \"vocab_size\": 28996\n",
            "}\n",
            "\n",
            "INFO:pytorch_pretrained_bert.modeling:Weights of BertForQuestionAnswering not initialized from pretrained model: ['qa_outputs.weight', 'qa_outputs.bias']\n",
            "INFO:pytorch_pretrained_bert.modeling:Weights from pretrained model not used in BertForQuestionAnswering: ['cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias']\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "BertForQuestionAnswering(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(28996, 768)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): BertLayerNorm()\n",
              "      (dropout): Dropout(p=0.1)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): BertLayerNorm()\n",
              "              (dropout): Dropout(p=0.1)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): BertLayerNorm()\n",
              "            (dropout): Dropout(p=0.1)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (qa_outputs): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JXk-rlmKTxn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Prepare optimizer\n",
        "param_optimizer = list(model.named_parameters())\n",
        "\n",
        "# hack to remove pooler, which is not used\n",
        "# thus it produce None grad that break apex\n",
        "param_optimizer = [n for n in param_optimizer if 'pooler' not in n[0]]\n",
        "\n",
        "no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
        "optimizer_grouped_parameters = [\n",
        "    {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': 0.01},\n",
        "    {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}\n",
        "    ]\n",
        "\n",
        "# skipping 911-922 not more than one gpu \n",
        "#skipping 937-952 bc not using 16 float position \n",
        "\n",
        "optimizer = BertAdam(optimizer_grouped_parameters,lr=learning_rate,\n",
        "                warmup=warmup_proportion,\n",
        "                t_total=num_train_optimization_steps)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7B2bnorN7_sI",
        "colab_type": "code",
        "outputId": "c03fbb2e-94d4-4be3-d6c3-9d93edaa7c31",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 4457
        }
      },
      "source": [
        "global_step = 0\n",
        "\n",
        "train_features = convert_examples_to_features(\n",
        "        examples=train_examples,\n",
        "        tokenizer=tokenizer,\n",
        "        max_seq_length=max_seq_length,\n",
        "        doc_stride=doc_stride,\n",
        "        max_query_length=max_query_length,\n",
        "        is_training=True)\n",
        "logger.info(\"***** Running training *****\")\n",
        "logger.info(\"  Num orig examples = %d\", len(train_examples))\n",
        "logger.info(\"  Num split examples = %d\", len(train_features))\n",
        "logger.info(\"  Batch size = %d\", train_batch_size)\n",
        "logger.info(\"  Num steps = %d\", num_train_optimization_steps)\n",
        "all_input_ids = torch.tensor([f.input_ids for f in train_features], dtype=torch.long)\n",
        "all_input_mask = torch.tensor([f.input_mask for f in train_features], dtype=torch.long)\n",
        "all_segment_ids = torch.tensor([f.segment_ids for f in train_features], dtype=torch.long)\n",
        "all_start_positions = torch.tensor([f.start_position for f in train_features], dtype=torch.long)\n",
        "all_end_positions = torch.tensor([f.end_position for f in train_features], dtype=torch.long)\n",
        "train_data = TensorDataset(all_input_ids, all_input_mask, all_segment_ids,\n",
        "                           all_start_positions, all_end_positions)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=train_batch_size)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000000\n",
            "INFO:run_squad:example_index: 0\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] When did Bey ##on ##ce start becoming popular ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 11:0 12:1 13:1 14:2 15:2 16:2 17:3 18:3 19:3 20:3 21:4 22:4 23:4 24:4 25:4 26:4 27:4 28:5 29:5 30:6 31:7 32:7 33:8 34:8 35:9 36:10 37:11 38:12 39:12 40:13 41:13 42:14 43:15 44:16 45:17 46:17 47:18 48:19 49:20 50:21 51:22 52:22 53:23 54:23 55:24 56:25 57:26 58:27 59:28 60:29 61:30 62:31 63:32 64:33 65:34 66:34 67:35 68:36 69:37 70:38 71:39 72:40 73:41 74:42 75:43 76:44 77:45 78:46 79:47 80:47 81:47 82:48 83:48 84:48 85:49 86:49 87:49 88:50 89:50 90:51 91:51 92:52 93:53 94:54 95:54 96:55 97:55 98:56 99:56 100:57 101:58 102:59 103:60 104:61 105:62 106:63 107:63 108:63 109:64 110:64 111:64 112:65 113:66 114:67 115:68 116:69 117:69 118:70 119:71 120:72 121:73 122:74 123:75 124:76 125:76 126:76 127:77 128:78 129:78 130:79 131:79 132:80 133:81 134:82 135:82 136:82 137:82 138:83 139:84 140:85 141:86 142:87 143:88 144:89 145:90 146:90 147:91 148:92 149:93 150:94 151:95 152:96 153:97 154:98 155:99 156:100 157:101 158:101 159:101 160:102 161:103 162:103 163:104 164:105 165:105 166:106 167:107 168:107 169:108 170:108 171:108\n",
            "INFO:run_squad:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True\n",
            "INFO:run_squad:input_ids: 101 1332 1225 24896 1320 2093 1838 2479 1927 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 71\n",
            "INFO:run_squad:end_position: 74\n",
            "INFO:run_squad:answer: in the late 1990s\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000001\n",
            "INFO:run_squad:example_index: 1\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] What areas did Bey ##on ##ce compete in when she was growing up ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 16:0 17:1 18:1 19:2 20:2 21:2 22:3 23:3 24:3 25:3 26:4 27:4 28:4 29:4 30:4 31:4 32:4 33:5 34:5 35:6 36:7 37:7 38:8 39:8 40:9 41:10 42:11 43:12 44:12 45:13 46:13 47:14 48:15 49:16 50:17 51:17 52:18 53:19 54:20 55:21 56:22 57:22 58:23 59:23 60:24 61:25 62:26 63:27 64:28 65:29 66:30 67:31 68:32 69:33 70:34 71:34 72:35 73:36 74:37 75:38 76:39 77:40 78:41 79:42 80:43 81:44 82:45 83:46 84:47 85:47 86:47 87:48 88:48 89:48 90:49 91:49 92:49 93:50 94:50 95:51 96:51 97:52 98:53 99:54 100:54 101:55 102:55 103:56 104:56 105:57 106:58 107:59 108:60 109:61 110:62 111:63 112:63 113:63 114:64 115:64 116:64 117:65 118:66 119:67 120:68 121:69 122:69 123:70 124:71 125:72 126:73 127:74 128:75 129:76 130:76 131:76 132:77 133:78 134:78 135:79 136:79 137:80 138:81 139:82 140:82 141:82 142:82 143:83 144:84 145:85 146:86 147:87 148:88 149:89 150:90 151:90 152:91 153:92 154:93 155:94 156:95 157:96 158:97 159:98 160:99 161:100 162:101 163:101 164:101 165:102 166:103 167:103 168:104 169:105 170:105 171:106 172:107 173:107 174:108 175:108 176:108\n",
            "INFO:run_squad:token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True\n",
            "INFO:run_squad:input_ids: 101 1327 1877 1225 24896 1320 2093 4845 1107 1165 1131 1108 2898 1146 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 64\n",
            "INFO:run_squad:end_position: 66\n",
            "INFO:run_squad:answer: singing and dancing\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000002\n",
            "INFO:run_squad:example_index: 2\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] When did Bey ##on ##ce leave Destiny ' s Child and become a solo singer ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 18:0 19:1 20:1 21:2 22:2 23:2 24:3 25:3 26:3 27:3 28:4 29:4 30:4 31:4 32:4 33:4 34:4 35:5 36:5 37:6 38:7 39:7 40:8 41:8 42:9 43:10 44:11 45:12 46:12 47:13 48:13 49:14 50:15 51:16 52:17 53:17 54:18 55:19 56:20 57:21 58:22 59:22 60:23 61:23 62:24 63:25 64:26 65:27 66:28 67:29 68:30 69:31 70:32 71:33 72:34 73:34 74:35 75:36 76:37 77:38 78:39 79:40 80:41 81:42 82:43 83:44 84:45 85:46 86:47 87:47 88:47 89:48 90:48 91:48 92:49 93:49 94:49 95:50 96:50 97:51 98:51 99:52 100:53 101:54 102:54 103:55 104:55 105:56 106:56 107:57 108:58 109:59 110:60 111:61 112:62 113:63 114:63 115:63 116:64 117:64 118:64 119:65 120:66 121:67 122:68 123:69 124:69 125:70 126:71 127:72 128:73 129:74 130:75 131:76 132:76 133:76 134:77 135:78 136:78 137:79 138:79 139:80 140:81 141:82 142:82 143:82 144:82 145:83 146:84 147:85 148:86 149:87 150:88 151:89 152:90 153:90 154:91 155:92 156:93 157:94 158:95 159:96 160:97 161:98 162:99 163:100 164:101 165:101 166:101 167:102 168:103 169:103 170:104 171:105 172:105 173:106 174:107 175:107 176:108 177:108 178:108\n",
            "INFO:run_squad:token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True\n",
            "INFO:run_squad:input_ids: 101 1332 1225 24896 1320 2093 1817 16784 112 188 6405 1105 1561 170 3444 2483 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 142\n",
            "INFO:run_squad:end_position: 142\n",
            "INFO:run_squad:answer: 2003\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000003\n",
            "INFO:run_squad:example_index: 3\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] In what city and state did Bey ##on ##ce grow up ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 14:0 15:1 16:1 17:2 18:2 19:2 20:3 21:3 22:3 23:3 24:4 25:4 26:4 27:4 28:4 29:4 30:4 31:5 32:5 33:6 34:7 35:7 36:8 37:8 38:9 39:10 40:11 41:12 42:12 43:13 44:13 45:14 46:15 47:16 48:17 49:17 50:18 51:19 52:20 53:21 54:22 55:22 56:23 57:23 58:24 59:25 60:26 61:27 62:28 63:29 64:30 65:31 66:32 67:33 68:34 69:34 70:35 71:36 72:37 73:38 74:39 75:40 76:41 77:42 78:43 79:44 80:45 81:46 82:47 83:47 84:47 85:48 86:48 87:48 88:49 89:49 90:49 91:50 92:50 93:51 94:51 95:52 96:53 97:54 98:54 99:55 100:55 101:56 102:56 103:57 104:58 105:59 106:60 107:61 108:62 109:63 110:63 111:63 112:64 113:64 114:64 115:65 116:66 117:67 118:68 119:69 120:69 121:70 122:71 123:72 124:73 125:74 126:75 127:76 128:76 129:76 130:77 131:78 132:78 133:79 134:79 135:80 136:81 137:82 138:82 139:82 140:82 141:83 142:84 143:85 144:86 145:87 146:88 147:89 148:90 149:90 150:91 151:92 152:93 153:94 154:95 155:96 156:97 157:98 158:99 159:100 160:101 161:101 162:101 163:102 164:103 165:103 166:104 167:105 168:105 169:106 170:107 171:107 172:108 173:108 174:108\n",
            "INFO:run_squad:token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "INFO:run_squad:input_ids: 101 1130 1184 1331 1105 1352 1225 24896 1320 2093 4328 1146 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 54\n",
            "INFO:run_squad:end_position: 56\n",
            "INFO:run_squad:answer: Houston , Texas\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000004\n",
            "INFO:run_squad:example_index: 4\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] In which decade did Bey ##on ##ce become famous ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 12:0 13:1 14:1 15:2 16:2 17:2 18:3 19:3 20:3 21:3 22:4 23:4 24:4 25:4 26:4 27:4 28:4 29:5 30:5 31:6 32:7 33:7 34:8 35:8 36:9 37:10 38:11 39:12 40:12 41:13 42:13 43:14 44:15 45:16 46:17 47:17 48:18 49:19 50:20 51:21 52:22 53:22 54:23 55:23 56:24 57:25 58:26 59:27 60:28 61:29 62:30 63:31 64:32 65:33 66:34 67:34 68:35 69:36 70:37 71:38 72:39 73:40 74:41 75:42 76:43 77:44 78:45 79:46 80:47 81:47 82:47 83:48 84:48 85:48 86:49 87:49 88:49 89:50 90:50 91:51 92:51 93:52 94:53 95:54 96:54 97:55 98:55 99:56 100:56 101:57 102:58 103:59 104:60 105:61 106:62 107:63 108:63 109:63 110:64 111:64 112:64 113:65 114:66 115:67 116:68 117:69 118:69 119:70 120:71 121:72 122:73 123:74 124:75 125:76 126:76 127:76 128:77 129:78 130:78 131:79 132:79 133:80 134:81 135:82 136:82 137:82 138:82 139:83 140:84 141:85 142:86 143:87 144:88 145:89 146:90 147:90 148:91 149:92 150:93 151:94 152:95 153:96 154:97 155:98 156:99 157:100 158:101 159:101 160:101 161:102 162:103 163:103 164:104 165:105 166:105 167:106 168:107 169:107 170:108 171:108 172:108\n",
            "INFO:run_squad:token_is_max_context: 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True\n",
            "INFO:run_squad:input_ids: 101 1130 1134 4967 1225 24896 1320 2093 1561 2505 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 74\n",
            "INFO:run_squad:end_position: 75\n",
            "INFO:run_squad:answer: late 1990s\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000005\n",
            "INFO:run_squad:example_index: 5\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] In what R & B group was she the lead singer ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 14:0 15:1 16:1 17:2 18:2 19:2 20:3 21:3 22:3 23:3 24:4 25:4 26:4 27:4 28:4 29:4 30:4 31:5 32:5 33:6 34:7 35:7 36:8 37:8 38:9 39:10 40:11 41:12 42:12 43:13 44:13 45:14 46:15 47:16 48:17 49:17 50:18 51:19 52:20 53:21 54:22 55:22 56:23 57:23 58:24 59:25 60:26 61:27 62:28 63:29 64:30 65:31 66:32 67:33 68:34 69:34 70:35 71:36 72:37 73:38 74:39 75:40 76:41 77:42 78:43 79:44 80:45 81:46 82:47 83:47 84:47 85:48 86:48 87:48 88:49 89:49 90:49 91:50 92:50 93:51 94:51 95:52 96:53 97:54 98:54 99:55 100:55 101:56 102:56 103:57 104:58 105:59 106:60 107:61 108:62 109:63 110:63 111:63 112:64 113:64 114:64 115:65 116:66 117:67 118:68 119:69 120:69 121:70 122:71 123:72 124:73 125:74 126:75 127:76 128:76 129:76 130:77 131:78 132:78 133:79 134:79 135:80 136:81 137:82 138:82 139:82 140:82 141:83 142:84 143:85 144:86 145:87 146:88 147:89 148:90 149:90 150:91 151:92 152:93 153:94 154:95 155:96 156:97 157:98 158:99 159:100 160:101 161:101 162:101 163:102 164:103 165:103 166:104 167:105 168:105 169:106 170:107 171:107 172:108 173:108 174:108\n",
            "INFO:run_squad:token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "INFO:run_squad:input_ids: 101 1130 1184 155 111 139 1372 1108 1131 1103 1730 2483 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 88\n",
            "INFO:run_squad:end_position: 91\n",
            "INFO:run_squad:answer: Destiny ' s Child\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000006\n",
            "INFO:run_squad:example_index: 6\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] What album made her a worldwide known artist ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 11:0 12:1 13:1 14:2 15:2 16:2 17:3 18:3 19:3 20:3 21:4 22:4 23:4 24:4 25:4 26:4 27:4 28:5 29:5 30:6 31:7 32:7 33:8 34:8 35:9 36:10 37:11 38:12 39:12 40:13 41:13 42:14 43:15 44:16 45:17 46:17 47:18 48:19 49:20 50:21 51:22 52:22 53:23 54:23 55:24 56:25 57:26 58:27 59:28 60:29 61:30 62:31 63:32 64:33 65:34 66:34 67:35 68:36 69:37 70:38 71:39 72:40 73:41 74:42 75:43 76:44 77:45 78:46 79:47 80:47 81:47 82:48 83:48 84:48 85:49 86:49 87:49 88:50 89:50 90:51 91:51 92:52 93:53 94:54 95:54 96:55 97:55 98:56 99:56 100:57 101:58 102:59 103:60 104:61 105:62 106:63 107:63 108:63 109:64 110:64 111:64 112:65 113:66 114:67 115:68 116:69 117:69 118:70 119:71 120:72 121:73 122:74 123:75 124:76 125:76 126:76 127:77 128:78 129:78 130:79 131:79 132:80 133:81 134:82 135:82 136:82 137:82 138:83 139:84 140:85 141:86 142:87 143:88 144:89 145:90 146:90 147:91 148:92 149:93 150:94 151:95 152:96 153:97 154:98 155:99 156:100 157:101 158:101 159:101 160:102 161:103 162:103 163:104 164:105 165:105 166:106 167:107 168:107 169:108 170:108 171:108\n",
            "INFO:run_squad:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True\n",
            "INFO:run_squad:input_ids: 101 1327 1312 1189 1123 170 4529 1227 2360 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 130\n",
            "INFO:run_squad:end_position: 133\n",
            "INFO:run_squad:answer: Dangerous ##ly in Love\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000007\n",
            "INFO:run_squad:example_index: 7\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] Who managed the Destiny ' s Child group ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 11:0 12:1 13:1 14:2 15:2 16:2 17:3 18:3 19:3 20:3 21:4 22:4 23:4 24:4 25:4 26:4 27:4 28:5 29:5 30:6 31:7 32:7 33:8 34:8 35:9 36:10 37:11 38:12 39:12 40:13 41:13 42:14 43:15 44:16 45:17 46:17 47:18 48:19 49:20 50:21 51:22 52:22 53:23 54:23 55:24 56:25 57:26 58:27 59:28 60:29 61:30 62:31 63:32 64:33 65:34 66:34 67:35 68:36 69:37 70:38 71:39 72:40 73:41 74:42 75:43 76:44 77:45 78:46 79:47 80:47 81:47 82:48 83:48 84:48 85:49 86:49 87:49 88:50 89:50 90:51 91:51 92:52 93:53 94:54 95:54 96:55 97:55 98:56 99:56 100:57 101:58 102:59 103:60 104:61 105:62 106:63 107:63 108:63 109:64 110:64 111:64 112:65 113:66 114:67 115:68 116:69 117:69 118:70 119:71 120:72 121:73 122:74 123:75 124:76 125:76 126:76 127:77 128:78 129:78 130:79 131:79 132:80 133:81 134:82 135:82 136:82 137:82 138:83 139:84 140:85 141:86 142:87 143:88 144:89 145:90 146:90 147:91 148:92 149:93 150:94 151:95 152:96 153:97 154:98 155:99 156:100 157:101 158:101 159:101 160:102 161:103 162:103 163:104 164:105 165:105 166:106 167:107 168:107 169:108 170:108 171:108\n",
            "INFO:run_squad:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True\n",
            "INFO:run_squad:input_ids: 101 2627 2374 1103 16784 112 188 6405 1372 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 96\n",
            "INFO:run_squad:end_position: 98\n",
            "INFO:run_squad:answer: Math ##ew Knowles\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000008\n",
            "INFO:run_squad:example_index: 8\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] When did Beyoncé rise to fame ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 9:0 10:1 11:1 12:2 13:2 14:2 15:3 16:3 17:3 18:3 19:4 20:4 21:4 22:4 23:4 24:4 25:4 26:5 27:5 28:6 29:7 30:7 31:8 32:8 33:9 34:10 35:11 36:12 37:12 38:13 39:13 40:14 41:15 42:16 43:17 44:17 45:18 46:19 47:20 48:21 49:22 50:22 51:23 52:23 53:24 54:25 55:26 56:27 57:28 58:29 59:30 60:31 61:32 62:33 63:34 64:34 65:35 66:36 67:37 68:38 69:39 70:40 71:41 72:42 73:43 74:44 75:45 76:46 77:47 78:47 79:47 80:48 81:48 82:48 83:49 84:49 85:49 86:50 87:50 88:51 89:51 90:52 91:53 92:54 93:54 94:55 95:55 96:56 97:56 98:57 99:58 100:59 101:60 102:61 103:62 104:63 105:63 106:63 107:64 108:64 109:64 110:65 111:66 112:67 113:68 114:69 115:69 116:70 117:71 118:72 119:73 120:74 121:75 122:76 123:76 124:76 125:77 126:78 127:78 128:79 129:79 130:80 131:81 132:82 133:82 134:82 135:82 136:83 137:84 138:85 139:86 140:87 141:88 142:89 143:90 144:90 145:91 146:92 147:93 148:94 149:95 150:96 151:97 152:98 153:99 154:100 155:101 156:101 157:101 158:102 159:103 160:103 161:104 162:105 163:105 164:106 165:107 166:107 167:108 168:108 169:108\n",
            "INFO:run_squad:token_is_max_context: 9:True 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True\n",
            "INFO:run_squad:input_ids: 101 1332 1225 24041 3606 1106 8408 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 71\n",
            "INFO:run_squad:end_position: 72\n",
            "INFO:run_squad:answer: late 1990s\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000009\n",
            "INFO:run_squad:example_index: 9\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] What role did Beyoncé have in Destiny ' s Child ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 13:0 14:1 15:1 16:2 17:2 18:2 19:3 20:3 21:3 22:3 23:4 24:4 25:4 26:4 27:4 28:4 29:4 30:5 31:5 32:6 33:7 34:7 35:8 36:8 37:9 38:10 39:11 40:12 41:12 42:13 43:13 44:14 45:15 46:16 47:17 48:17 49:18 50:19 51:20 52:21 53:22 54:22 55:23 56:23 57:24 58:25 59:26 60:27 61:28 62:29 63:30 64:31 65:32 66:33 67:34 68:34 69:35 70:36 71:37 72:38 73:39 74:40 75:41 76:42 77:43 78:44 79:45 80:46 81:47 82:47 83:47 84:48 85:48 86:48 87:49 88:49 89:49 90:50 91:50 92:51 93:51 94:52 95:53 96:54 97:54 98:55 99:55 100:56 101:56 102:57 103:58 104:59 105:60 106:61 107:62 108:63 109:63 110:63 111:64 112:64 113:64 114:65 115:66 116:67 117:68 118:69 119:69 120:70 121:71 122:72 123:73 124:74 125:75 126:76 127:76 128:76 129:77 130:78 131:78 132:79 133:79 134:80 135:81 136:82 137:82 138:82 139:82 140:83 141:84 142:85 143:86 144:87 145:88 146:89 147:90 148:90 149:91 150:92 151:93 152:94 153:95 154:96 155:97 156:98 157:99 158:100 159:101 160:101 161:101 162:102 163:103 164:103 165:104 166:105 167:105 168:106 169:107 170:107 171:108 172:108 173:108\n",
            "INFO:run_squad:token_is_max_context: 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True\n",
            "INFO:run_squad:input_ids: 101 1327 1648 1225 24041 1138 1107 16784 112 188 6405 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 78\n",
            "INFO:run_squad:end_position: 79\n",
            "INFO:run_squad:answer: lead singer\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000010\n",
            "INFO:run_squad:example_index: 10\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] What was the first album Beyoncé released as a solo artist ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 14:0 15:1 16:1 17:2 18:2 19:2 20:3 21:3 22:3 23:3 24:4 25:4 26:4 27:4 28:4 29:4 30:4 31:5 32:5 33:6 34:7 35:7 36:8 37:8 38:9 39:10 40:11 41:12 42:12 43:13 44:13 45:14 46:15 47:16 48:17 49:17 50:18 51:19 52:20 53:21 54:22 55:22 56:23 57:23 58:24 59:25 60:26 61:27 62:28 63:29 64:30 65:31 66:32 67:33 68:34 69:34 70:35 71:36 72:37 73:38 74:39 75:40 76:41 77:42 78:43 79:44 80:45 81:46 82:47 83:47 84:47 85:48 86:48 87:48 88:49 89:49 90:49 91:50 92:50 93:51 94:51 95:52 96:53 97:54 98:54 99:55 100:55 101:56 102:56 103:57 104:58 105:59 106:60 107:61 108:62 109:63 110:63 111:63 112:64 113:64 114:64 115:65 116:66 117:67 118:68 119:69 120:69 121:70 122:71 123:72 124:73 125:74 126:75 127:76 128:76 129:76 130:77 131:78 132:78 133:79 134:79 135:80 136:81 137:82 138:82 139:82 140:82 141:83 142:84 143:85 144:86 145:87 146:88 147:89 148:90 149:90 150:91 151:92 152:93 153:94 154:95 155:96 156:97 157:98 158:99 159:100 160:101 161:101 162:101 163:102 164:103 165:103 166:104 167:105 168:105 169:106 170:107 171:107 172:108 173:108 174:108\n",
            "INFO:run_squad:token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "INFO:run_squad:input_ids: 101 1327 1108 1103 1148 1312 24041 1308 1112 170 3444 2360 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 133\n",
            "INFO:run_squad:end_position: 136\n",
            "INFO:run_squad:answer: Dangerous ##ly in Love\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000011\n",
            "INFO:run_squad:example_index: 11\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] When did Beyoncé release Dangerous ##ly in Love ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 11:0 12:1 13:1 14:2 15:2 16:2 17:3 18:3 19:3 20:3 21:4 22:4 23:4 24:4 25:4 26:4 27:4 28:5 29:5 30:6 31:7 32:7 33:8 34:8 35:9 36:10 37:11 38:12 39:12 40:13 41:13 42:14 43:15 44:16 45:17 46:17 47:18 48:19 49:20 50:21 51:22 52:22 53:23 54:23 55:24 56:25 57:26 58:27 59:28 60:29 61:30 62:31 63:32 64:33 65:34 66:34 67:35 68:36 69:37 70:38 71:39 72:40 73:41 74:42 75:43 76:44 77:45 78:46 79:47 80:47 81:47 82:48 83:48 84:48 85:49 86:49 87:49 88:50 89:50 90:51 91:51 92:52 93:53 94:54 95:54 96:55 97:55 98:56 99:56 100:57 101:58 102:59 103:60 104:61 105:62 106:63 107:63 108:63 109:64 110:64 111:64 112:65 113:66 114:67 115:68 116:69 117:69 118:70 119:71 120:72 121:73 122:74 123:75 124:76 125:76 126:76 127:77 128:78 129:78 130:79 131:79 132:80 133:81 134:82 135:82 136:82 137:82 138:83 139:84 140:85 141:86 142:87 143:88 144:89 145:90 146:90 147:91 148:92 149:93 150:94 151:95 152:96 153:97 154:98 155:99 156:100 157:101 158:101 159:101 160:102 161:103 162:103 163:104 164:105 165:105 166:106 167:107 168:107 169:108 170:108 171:108\n",
            "INFO:run_squad:token_is_max_context: 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True\n",
            "INFO:run_squad:input_ids: 101 1332 1225 24041 1836 20924 1193 1107 2185 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 135\n",
            "INFO:run_squad:end_position: 135\n",
            "INFO:run_squad:answer: 2003\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000012\n",
            "INFO:run_squad:example_index: 12\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] How many Grammy awards did Beyoncé win for her first solo album ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 15:0 16:1 17:1 18:2 19:2 20:2 21:3 22:3 23:3 24:3 25:4 26:4 27:4 28:4 29:4 30:4 31:4 32:5 33:5 34:6 35:7 36:7 37:8 38:8 39:9 40:10 41:11 42:12 43:12 44:13 45:13 46:14 47:15 48:16 49:17 50:17 51:18 52:19 53:20 54:21 55:22 56:22 57:23 58:23 59:24 60:25 61:26 62:27 63:28 64:29 65:30 66:31 67:32 68:33 69:34 70:34 71:35 72:36 73:37 74:38 75:39 76:40 77:41 78:42 79:43 80:44 81:45 82:46 83:47 84:47 85:47 86:48 87:48 88:48 89:49 90:49 91:49 92:50 93:50 94:51 95:51 96:52 97:53 98:54 99:54 100:55 101:55 102:56 103:56 104:57 105:58 106:59 107:60 108:61 109:62 110:63 111:63 112:63 113:64 114:64 115:64 116:65 117:66 118:67 119:68 120:69 121:69 122:70 123:71 124:72 125:73 126:74 127:75 128:76 129:76 130:76 131:77 132:78 133:78 134:79 135:79 136:80 137:81 138:82 139:82 140:82 141:82 142:83 143:84 144:85 145:86 146:87 147:88 148:89 149:90 150:90 151:91 152:92 153:93 154:94 155:95 156:96 157:97 158:98 159:99 160:100 161:101 162:101 163:101 164:102 165:103 166:103 167:104 168:105 169:105 170:106 171:107 172:107 173:108 174:108 175:108\n",
            "INFO:run_squad:token_is_max_context: 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True\n",
            "INFO:run_squad:input_ids: 101 1731 1242 8645 3745 1225 24041 1782 1111 1123 1148 3444 1312 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 152\n",
            "INFO:run_squad:end_position: 152\n",
            "INFO:run_squad:answer: five\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000013\n",
            "INFO:run_squad:example_index: 13\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] What was Beyoncé ' s role in Destiny ' s Child ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 14:0 15:1 16:1 17:2 18:2 19:2 20:3 21:3 22:3 23:3 24:4 25:4 26:4 27:4 28:4 29:4 30:4 31:5 32:5 33:6 34:7 35:7 36:8 37:8 38:9 39:10 40:11 41:12 42:12 43:13 44:13 45:14 46:15 47:16 48:17 49:17 50:18 51:19 52:20 53:21 54:22 55:22 56:23 57:23 58:24 59:25 60:26 61:27 62:28 63:29 64:30 65:31 66:32 67:33 68:34 69:34 70:35 71:36 72:37 73:38 74:39 75:40 76:41 77:42 78:43 79:44 80:45 81:46 82:47 83:47 84:47 85:48 86:48 87:48 88:49 89:49 90:49 91:50 92:50 93:51 94:51 95:52 96:53 97:54 98:54 99:55 100:55 101:56 102:56 103:57 104:58 105:59 106:60 107:61 108:62 109:63 110:63 111:63 112:64 113:64 114:64 115:65 116:66 117:67 118:68 119:69 120:69 121:70 122:71 123:72 124:73 125:74 126:75 127:76 128:76 129:76 130:77 131:78 132:78 133:79 134:79 135:80 136:81 137:82 138:82 139:82 140:82 141:83 142:84 143:85 144:86 145:87 146:88 147:89 148:90 149:90 150:91 151:92 152:93 153:94 154:95 155:96 156:97 157:98 158:99 159:100 160:101 161:101 162:101 163:102 164:103 165:103 166:104 167:105 168:105 169:106 170:107 171:107 172:108 173:108 174:108\n",
            "INFO:run_squad:token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "INFO:run_squad:input_ids: 101 1327 1108 24041 112 188 1648 1107 16784 112 188 6405 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 79\n",
            "INFO:run_squad:end_position: 80\n",
            "INFO:run_squad:answer: lead singer\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000014\n",
            "INFO:run_squad:example_index: 14\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] What was the name of Beyoncé ' s first solo album ? [SEP] Beyoncé G ##iselle Knowles - Carter ( / [UNK] / bee - Y ##ON - say ) ( born September 4 , 1981 ) is an American singer , songwriter , record producer and actress . Born and raised in Houston , Texas , she performed in various singing and dancing competitions as a child , and rose to fame in the late 1990s as lead singer of R & B girl - group Destiny ' s Child . Man ##aged by her father , Math ##ew Knowles , the group became one of the world ' s best - selling girl groups of all time . Their hiatus saw the release of Beyoncé ' s debut album , Dangerous ##ly in Love ( 2003 ) , which established her as a solo artist worldwide , earned five Grammy Awards and featured the Billboard Hot 100 number - one singles \" Crazy in Love \" and \" Baby Boy \" . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 14:0 15:1 16:1 17:2 18:2 19:2 20:3 21:3 22:3 23:3 24:4 25:4 26:4 27:4 28:4 29:4 30:4 31:5 32:5 33:6 34:7 35:7 36:8 37:8 38:9 39:10 40:11 41:12 42:12 43:13 44:13 45:14 46:15 47:16 48:17 49:17 50:18 51:19 52:20 53:21 54:22 55:22 56:23 57:23 58:24 59:25 60:26 61:27 62:28 63:29 64:30 65:31 66:32 67:33 68:34 69:34 70:35 71:36 72:37 73:38 74:39 75:40 76:41 77:42 78:43 79:44 80:45 81:46 82:47 83:47 84:47 85:48 86:48 87:48 88:49 89:49 90:49 91:50 92:50 93:51 94:51 95:52 96:53 97:54 98:54 99:55 100:55 101:56 102:56 103:57 104:58 105:59 106:60 107:61 108:62 109:63 110:63 111:63 112:64 113:64 114:64 115:65 116:66 117:67 118:68 119:69 120:69 121:70 122:71 123:72 124:73 125:74 126:75 127:76 128:76 129:76 130:77 131:78 132:78 133:79 134:79 135:80 136:81 137:82 138:82 139:82 140:82 141:83 142:84 143:85 144:86 145:87 146:88 147:89 148:90 149:90 150:91 151:92 152:93 153:94 154:95 155:96 156:97 157:98 158:99 159:100 160:101 161:101 162:101 163:102 164:103 165:103 166:104 167:105 168:105 169:106 170:107 171:107 172:108 173:108 174:108\n",
            "INFO:run_squad:token_is_max_context: 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True\n",
            "INFO:run_squad:input_ids: 101 1327 1108 1103 1271 1104 24041 112 188 1148 3444 1312 136 102 24041 144 22080 25384 118 5007 113 120 100 120 17775 118 162 11414 118 1474 114 113 1255 1347 125 117 2358 114 1110 1126 1237 2483 117 5523 117 1647 2451 1105 3647 119 3526 1105 2120 1107 4666 117 2245 117 1131 1982 1107 1672 4241 1105 5923 6025 1112 170 2027 117 1105 3152 1106 8408 1107 1103 1523 3281 1112 1730 2483 1104 155 111 139 1873 118 1372 16784 112 188 6405 119 2268 15841 1118 1123 1401 117 15112 5773 25384 117 1103 1372 1245 1141 1104 1103 1362 112 188 1436 118 4147 1873 2114 1104 1155 1159 119 2397 14938 1486 1103 1836 1104 24041 112 188 1963 1312 117 20924 1193 1107 2185 113 1581 114 117 1134 1628 1123 1112 170 3444 2360 4529 117 2829 1421 8645 2763 1105 2081 1103 4192 4126 1620 1295 118 1141 3896 107 11722 1107 2185 107 1105 107 6008 4596 107 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 133\n",
            "INFO:run_squad:end_position: 136\n",
            "INFO:run_squad:answer: Dangerous ##ly in Love\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000015\n",
            "INFO:run_squad:example_index: 15\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] After her second solo album , what other entertainment venture did Bey ##on ##ce explore ? [SEP] Following the di ##sband ##ment of Destiny ' s Child in June 2005 , she released her second solo album , B ' Day ( 2006 ) , which contained hits \" D ##é ##j ##à V ##u \" , \" I ##rre ##place ##able \" , and \" Beautiful Lia ##r \" . Beyoncé also ventured into acting , with a Golden Globe - nominated performance in Dream ##girl ##s ( 2006 ) , and starring roles in The Pink Panther ( 2006 ) and O ##bs ##essed ( 2009 ) . Her marriage to rapper Jay Z and portrayal of E ##tta James in Cadillac Records ( 2008 ) influenced her third album , I Am . . . Sasha Fi ##er ##ce ( 2008 ) , which saw the birth of her alter - ego Sasha Fi ##er ##ce and earned a record - setting six Grammy Awards in 2010 , including Song of the Year for \" Single Ladies ( Put a Ring on It ) \" . Beyoncé took a hiatus from music in 2010 and took over management of her career ; her fourth album 4 ( 2011 ) was subsequently me ##llow ##er in tone , exploring 1970s funk , 1980s pop , and 1990s soul . Her critically acclaimed fifth studio album , Beyoncé ( 2013 ) , was distinguished from previous releases by its experimental production and exploration of darker themes . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 18:0 19:1 20:2 21:2 22:2 23:3 24:4 25:4 26:4 27:5 28:6 29:7 30:8 31:8 32:9 33:10 34:11 35:12 36:13 37:14 38:14 39:15 40:15 41:15 42:16 43:16 44:16 45:16 46:17 47:18 48:19 49:20 50:20 51:20 52:20 53:20 54:21 55:21 56:21 57:21 58:22 59:22 60:22 61:22 62:22 63:22 64:22 65:23 66:24 67:24 68:25 69:25 70:25 71:25 72:26 73:27 74:28 75:29 76:30 77:30 78:31 79:32 80:33 81:34 82:34 83:34 84:35 85:36 86:37 87:37 88:37 89:38 90:38 91:38 92:38 93:39 94:40 95:41 96:42 97:43 98:44 99:45 100:46 101:46 102:46 103:47 104:48 105:48 106:48 107:49 108:49 109:49 110:49 111:50 112:51 113:52 114:53 115:54 116:55 117:56 118:57 119:58 120:59 121:59 122:60 123:61 124:62 125:63 126:64 127:64 128:64 129:65 130:66 131:67 132:68 133:68 134:69 135:70 136:70 137:70 138:70 139:71 140:72 141:72 142:72 143:73 144:73 145:73 146:73 147:74 148:75 149:76 150:77 151:78 152:79 153:80 154:80 155:80 156:81 157:82 158:82 159:82 160:83 161:84 162:85 163:86 164:86 165:86 166:87 167:88 168:89 169:90 170:91 171:91 172:92 173:93 174:94 175:95 176:96 177:97 178:98 179:98 180:99 181:100 182:100 183:101 184:102 185:103 186:104 187:104 188:104 189:104 190:105 191:106 192:107 193:108 194:109 195:110 196:111 197:112 198:113 199:114 200:115 201:116 202:117 203:118 204:119 205:119 206:120 207:121 208:122 209:123 210:124 211:124 212:124 213:125 214:126 215:127 216:127 217:127 218:128 219:129 220:129 221:130 222:131 223:132 224:132 225:133 226:134 227:134 228:135 229:136 230:137 231:137 232:138 233:139 234:140 235:141 236:142 237:143 238:143 239:144 240:145 241:145 242:145 243:145 244:146 245:147 246:148 247:149 248:150 249:151 250:152 251:153 252:154 253:155 254:156 255:157 256:158 257:159 258:159\n",
            "INFO:run_squad:token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True\n",
            "INFO:run_squad:input_ids: 101 1258 1123 1248 3444 1312 117 1184 1168 5936 7006 1225 24896 1320 2093 8664 136 102 2485 1103 4267 22309 1880 1104 16784 112 188 6405 1107 1340 1478 117 1131 1308 1123 1248 3444 1312 117 139 112 2295 113 1386 114 117 1134 4049 4919 107 141 2744 3361 9183 159 1358 107 117 107 146 11604 11256 1895 107 117 1105 107 9896 25760 1197 107 119 24041 1145 22542 1154 3176 117 1114 170 3684 10421 118 3639 2099 1107 6525 17001 1116 113 1386 114 117 1105 3937 3573 1107 1109 10763 19719 113 1386 114 1105 152 4832 24940 113 1371 114 119 1430 2742 1106 11333 5525 163 1105 14513 1104 142 5100 1600 1107 22393 2151 113 1369 114 4401 1123 1503 1312 117 146 7277 119 119 119 15327 17355 1200 2093 113 1369 114 117 1134 1486 1103 3485 1104 1123 13000 118 15550 15327 17355 1200 2093 1105 2829 170 1647 118 3545 1565 8645 2763 1107 1333 117 1259 3765 1104 1103 2381 1111 107 8353 10504 113 11913 170 8467 1113 1135 114 107 119 24041 1261 170 14938 1121 1390 1107 1333 1105 1261 1166 2635 1104 1123 1578 132 1123 2223 1312 125 113 1349 114 1108 2886 1143 24834 1200 1107 3586 117 12138 3095 19657 117 3011 3618 117 1105 3281 3960 119 1430 11774 10214 3049 2362 1312 117 24041 113 1381 114 117 1108 6019 1121 2166 6596 1118 1157 6700 1707 1105 10016 1104 9934 6621 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 76\n",
            "INFO:run_squad:end_position: 76\n",
            "INFO:run_squad:answer: acting\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000016\n",
            "INFO:run_squad:example_index: 16\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] Which artist did Bey ##on ##ce marry ? [SEP] Following the di ##sband ##ment of Destiny ' s Child in June 2005 , she released her second solo album , B ' Day ( 2006 ) , which contained hits \" D ##é ##j ##à V ##u \" , \" I ##rre ##place ##able \" , and \" Beautiful Lia ##r \" . Beyoncé also ventured into acting , with a Golden Globe - nominated performance in Dream ##girl ##s ( 2006 ) , and starring roles in The Pink Panther ( 2006 ) and O ##bs ##essed ( 2009 ) . Her marriage to rapper Jay Z and portrayal of E ##tta James in Cadillac Records ( 2008 ) influenced her third album , I Am . . . Sasha Fi ##er ##ce ( 2008 ) , which saw the birth of her alter - ego Sasha Fi ##er ##ce and earned a record - setting six Grammy Awards in 2010 , including Song of the Year for \" Single Ladies ( Put a Ring on It ) \" . Beyoncé took a hiatus from music in 2010 and took over management of her career ; her fourth album 4 ( 2011 ) was subsequently me ##llow ##er in tone , exploring 1970s funk , 1980s pop , and 1990s soul . Her critically acclaimed fifth studio album , Beyoncé ( 2013 ) , was distinguished from previous releases by its experimental production and exploration of darker themes . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 10:0 11:1 12:2 13:2 14:2 15:3 16:4 17:4 18:4 19:5 20:6 21:7 22:8 23:8 24:9 25:10 26:11 27:12 28:13 29:14 30:14 31:15 32:15 33:15 34:16 35:16 36:16 37:16 38:17 39:18 40:19 41:20 42:20 43:20 44:20 45:20 46:21 47:21 48:21 49:21 50:22 51:22 52:22 53:22 54:22 55:22 56:22 57:23 58:24 59:24 60:25 61:25 62:25 63:25 64:26 65:27 66:28 67:29 68:30 69:30 70:31 71:32 72:33 73:34 74:34 75:34 76:35 77:36 78:37 79:37 80:37 81:38 82:38 83:38 84:38 85:39 86:40 87:41 88:42 89:43 90:44 91:45 92:46 93:46 94:46 95:47 96:48 97:48 98:48 99:49 100:49 101:49 102:49 103:50 104:51 105:52 106:53 107:54 108:55 109:56 110:57 111:58 112:59 113:59 114:60 115:61 116:62 117:63 118:64 119:64 120:64 121:65 122:66 123:67 124:68 125:68 126:69 127:70 128:70 129:70 130:70 131:71 132:72 133:72 134:72 135:73 136:73 137:73 138:73 139:74 140:75 141:76 142:77 143:78 144:79 145:80 146:80 147:80 148:81 149:82 150:82 151:82 152:83 153:84 154:85 155:86 156:86 157:86 158:87 159:88 160:89 161:90 162:91 163:91 164:92 165:93 166:94 167:95 168:96 169:97 170:98 171:98 172:99 173:100 174:100 175:101 176:102 177:103 178:104 179:104 180:104 181:104 182:105 183:106 184:107 185:108 186:109 187:110 188:111 189:112 190:113 191:114 192:115 193:116 194:117 195:118 196:119 197:119 198:120 199:121 200:122 201:123 202:124 203:124 204:124 205:125 206:126 207:127 208:127 209:127 210:128 211:129 212:129 213:130 214:131 215:132 216:132 217:133 218:134 219:134 220:135 221:136 222:137 223:137 224:138 225:139 226:140 227:141 228:142 229:143 230:143 231:144 232:145 233:145 234:145 235:145 236:146 237:147 238:148 239:149 240:150 241:151 242:152 243:153 244:154 245:155 246:156 247:157 248:158 249:159 250:159\n",
            "INFO:run_squad:token_is_max_context: 10:True 11:True 12:True 13:True 14:True 15:True 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True\n",
            "INFO:run_squad:input_ids: 101 5979 2360 1225 24896 1320 2093 5390 136 102 2485 1103 4267 22309 1880 1104 16784 112 188 6405 1107 1340 1478 117 1131 1308 1123 1248 3444 1312 117 139 112 2295 113 1386 114 117 1134 4049 4919 107 141 2744 3361 9183 159 1358 107 117 107 146 11604 11256 1895 107 117 1105 107 9896 25760 1197 107 119 24041 1145 22542 1154 3176 117 1114 170 3684 10421 118 3639 2099 1107 6525 17001 1116 113 1386 114 117 1105 3937 3573 1107 1109 10763 19719 113 1386 114 1105 152 4832 24940 113 1371 114 119 1430 2742 1106 11333 5525 163 1105 14513 1104 142 5100 1600 1107 22393 2151 113 1369 114 4401 1123 1503 1312 117 146 7277 119 119 119 15327 17355 1200 2093 113 1369 114 117 1134 1486 1103 3485 1104 1123 13000 118 15550 15327 17355 1200 2093 1105 2829 170 1647 118 3545 1565 8645 2763 1107 1333 117 1259 3765 1104 1103 2381 1111 107 8353 10504 113 11913 170 8467 1113 1135 114 107 119 24041 1261 170 14938 1121 1390 1107 1333 1105 1261 1166 2635 1104 1123 1578 132 1123 2223 1312 125 113 1349 114 1108 2886 1143 24834 1200 1107 3586 117 12138 3095 19657 117 3011 3618 117 1105 3281 3960 119 1430 11774 10214 3049 2362 1312 117 24041 113 1381 114 117 1108 6019 1121 2166 6596 1118 1157 6700 1707 1105 10016 1104 9934 6621 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 107\n",
            "INFO:run_squad:end_position: 108\n",
            "INFO:run_squad:answer: Jay Z\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000017\n",
            "INFO:run_squad:example_index: 17\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] To set the record for Grammy ##s , how many did Bey ##on ##ce win ? [SEP] Following the di ##sband ##ment of Destiny ' s Child in June 2005 , she released her second solo album , B ' Day ( 2006 ) , which contained hits \" D ##é ##j ##à V ##u \" , \" I ##rre ##place ##able \" , and \" Beautiful Lia ##r \" . Beyoncé also ventured into acting , with a Golden Globe - nominated performance in Dream ##girl ##s ( 2006 ) , and starring roles in The Pink Panther ( 2006 ) and O ##bs ##essed ( 2009 ) . Her marriage to rapper Jay Z and portrayal of E ##tta James in Cadillac Records ( 2008 ) influenced her third album , I Am . . . Sasha Fi ##er ##ce ( 2008 ) , which saw the birth of her alter - ego Sasha Fi ##er ##ce and earned a record - setting six Grammy Awards in 2010 , including Song of the Year for \" Single Ladies ( Put a Ring on It ) \" . Beyoncé took a hiatus from music in 2010 and took over management of her career ; her fourth album 4 ( 2011 ) was subsequently me ##llow ##er in tone , exploring 1970s funk , 1980s pop , and 1990s soul . Her critically acclaimed fifth studio album , Beyoncé ( 2013 ) , was distinguished from previous releases by its experimental production and exploration of darker themes . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 18:0 19:1 20:2 21:2 22:2 23:3 24:4 25:4 26:4 27:5 28:6 29:7 30:8 31:8 32:9 33:10 34:11 35:12 36:13 37:14 38:14 39:15 40:15 41:15 42:16 43:16 44:16 45:16 46:17 47:18 48:19 49:20 50:20 51:20 52:20 53:20 54:21 55:21 56:21 57:21 58:22 59:22 60:22 61:22 62:22 63:22 64:22 65:23 66:24 67:24 68:25 69:25 70:25 71:25 72:26 73:27 74:28 75:29 76:30 77:30 78:31 79:32 80:33 81:34 82:34 83:34 84:35 85:36 86:37 87:37 88:37 89:38 90:38 91:38 92:38 93:39 94:40 95:41 96:42 97:43 98:44 99:45 100:46 101:46 102:46 103:47 104:48 105:48 106:48 107:49 108:49 109:49 110:49 111:50 112:51 113:52 114:53 115:54 116:55 117:56 118:57 119:58 120:59 121:59 122:60 123:61 124:62 125:63 126:64 127:64 128:64 129:65 130:66 131:67 132:68 133:68 134:69 135:70 136:70 137:70 138:70 139:71 140:72 141:72 142:72 143:73 144:73 145:73 146:73 147:74 148:75 149:76 150:77 151:78 152:79 153:80 154:80 155:80 156:81 157:82 158:82 159:82 160:83 161:84 162:85 163:86 164:86 165:86 166:87 167:88 168:89 169:90 170:91 171:91 172:92 173:93 174:94 175:95 176:96 177:97 178:98 179:98 180:99 181:100 182:100 183:101 184:102 185:103 186:104 187:104 188:104 189:104 190:105 191:106 192:107 193:108 194:109 195:110 196:111 197:112 198:113 199:114 200:115 201:116 202:117 203:118 204:119 205:119 206:120 207:121 208:122 209:123 210:124 211:124 212:124 213:125 214:126 215:127 216:127 217:127 218:128 219:129 220:129 221:130 222:131 223:132 224:132 225:133 226:134 227:134 228:135 229:136 230:137 231:137 232:138 233:139 234:140 235:141 236:142 237:143 238:143 239:144 240:145 241:145 242:145 243:145 244:146 245:147 246:148 247:149 248:150 249:151 250:152 251:153 252:154 253:155 254:156 255:157 256:158 257:159 258:159\n",
            "INFO:run_squad:token_is_max_context: 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True\n",
            "INFO:run_squad:input_ids: 101 1706 1383 1103 1647 1111 8645 1116 117 1293 1242 1225 24896 1320 2093 1782 136 102 2485 1103 4267 22309 1880 1104 16784 112 188 6405 1107 1340 1478 117 1131 1308 1123 1248 3444 1312 117 139 112 2295 113 1386 114 117 1134 4049 4919 107 141 2744 3361 9183 159 1358 107 117 107 146 11604 11256 1895 107 117 1105 107 9896 25760 1197 107 119 24041 1145 22542 1154 3176 117 1114 170 3684 10421 118 3639 2099 1107 6525 17001 1116 113 1386 114 117 1105 3937 3573 1107 1109 10763 19719 113 1386 114 1105 152 4832 24940 113 1371 114 119 1430 2742 1106 11333 5525 163 1105 14513 1104 142 5100 1600 1107 22393 2151 113 1369 114 4401 1123 1503 1312 117 146 7277 119 119 119 15327 17355 1200 2093 113 1369 114 117 1134 1486 1103 3485 1104 1123 13000 118 15550 15327 17355 1200 2093 1105 2829 170 1647 118 3545 1565 8645 2763 1107 1333 117 1259 3765 1104 1103 2381 1111 107 8353 10504 113 11913 170 8467 1113 1135 114 107 119 24041 1261 170 14938 1121 1390 1107 1333 1105 1261 1166 2635 1104 1123 1578 132 1123 2223 1312 125 113 1349 114 1108 2886 1143 24834 1200 1107 3586 117 12138 3095 19657 117 3011 3618 117 1105 3281 3960 119 1430 11774 10214 3049 2362 1312 117 24041 113 1381 114 117 1108 6019 1121 2166 6596 1118 1157 6700 1707 1105 10016 1104 9934 6621 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 166\n",
            "INFO:run_squad:end_position: 166\n",
            "INFO:run_squad:answer: six\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000018\n",
            "INFO:run_squad:example_index: 18\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] For what movie did Bey ##on ##ce receive her first Golden Globe nomination ? [SEP] Following the di ##sband ##ment of Destiny ' s Child in June 2005 , she released her second solo album , B ' Day ( 2006 ) , which contained hits \" D ##é ##j ##à V ##u \" , \" I ##rre ##place ##able \" , and \" Beautiful Lia ##r \" . Beyoncé also ventured into acting , with a Golden Globe - nominated performance in Dream ##girl ##s ( 2006 ) , and starring roles in The Pink Panther ( 2006 ) and O ##bs ##essed ( 2009 ) . Her marriage to rapper Jay Z and portrayal of E ##tta James in Cadillac Records ( 2008 ) influenced her third album , I Am . . . Sasha Fi ##er ##ce ( 2008 ) , which saw the birth of her alter - ego Sasha Fi ##er ##ce and earned a record - setting six Grammy Awards in 2010 , including Song of the Year for \" Single Ladies ( Put a Ring on It ) \" . Beyoncé took a hiatus from music in 2010 and took over management of her career ; her fourth album 4 ( 2011 ) was subsequently me ##llow ##er in tone , exploring 1970s funk , 1980s pop , and 1990s soul . Her critically acclaimed fifth studio album , Beyoncé ( 2013 ) , was distinguished from previous releases by its experimental production and exploration of darker themes . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 16:0 17:1 18:2 19:2 20:2 21:3 22:4 23:4 24:4 25:5 26:6 27:7 28:8 29:8 30:9 31:10 32:11 33:12 34:13 35:14 36:14 37:15 38:15 39:15 40:16 41:16 42:16 43:16 44:17 45:18 46:19 47:20 48:20 49:20 50:20 51:20 52:21 53:21 54:21 55:21 56:22 57:22 58:22 59:22 60:22 61:22 62:22 63:23 64:24 65:24 66:25 67:25 68:25 69:25 70:26 71:27 72:28 73:29 74:30 75:30 76:31 77:32 78:33 79:34 80:34 81:34 82:35 83:36 84:37 85:37 86:37 87:38 88:38 89:38 90:38 91:39 92:40 93:41 94:42 95:43 96:44 97:45 98:46 99:46 100:46 101:47 102:48 103:48 104:48 105:49 106:49 107:49 108:49 109:50 110:51 111:52 112:53 113:54 114:55 115:56 116:57 117:58 118:59 119:59 120:60 121:61 122:62 123:63 124:64 125:64 126:64 127:65 128:66 129:67 130:68 131:68 132:69 133:70 134:70 135:70 136:70 137:71 138:72 139:72 140:72 141:73 142:73 143:73 144:73 145:74 146:75 147:76 148:77 149:78 150:79 151:80 152:80 153:80 154:81 155:82 156:82 157:82 158:83 159:84 160:85 161:86 162:86 163:86 164:87 165:88 166:89 167:90 168:91 169:91 170:92 171:93 172:94 173:95 174:96 175:97 176:98 177:98 178:99 179:100 180:100 181:101 182:102 183:103 184:104 185:104 186:104 187:104 188:105 189:106 190:107 191:108 192:109 193:110 194:111 195:112 196:113 197:114 198:115 199:116 200:117 201:118 202:119 203:119 204:120 205:121 206:122 207:123 208:124 209:124 210:124 211:125 212:126 213:127 214:127 215:127 216:128 217:129 218:129 219:130 220:131 221:132 222:132 223:133 224:134 225:134 226:135 227:136 228:137 229:137 230:138 231:139 232:140 233:141 234:142 235:143 236:143 237:144 238:145 239:145 240:145 241:145 242:146 243:147 244:148 245:149 246:150 247:151 248:152 249:153 250:154 251:155 252:156 253:157 254:158 255:159 256:159\n",
            "INFO:run_squad:token_is_max_context: 16:True 17:True 18:True 19:True 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True\n",
            "INFO:run_squad:input_ids: 101 1370 1184 2523 1225 24896 1320 2093 3531 1123 1148 3684 10421 6294 136 102 2485 1103 4267 22309 1880 1104 16784 112 188 6405 1107 1340 1478 117 1131 1308 1123 1248 3444 1312 117 139 112 2295 113 1386 114 117 1134 4049 4919 107 141 2744 3361 9183 159 1358 107 117 107 146 11604 11256 1895 107 117 1105 107 9896 25760 1197 107 119 24041 1145 22542 1154 3176 117 1114 170 3684 10421 118 3639 2099 1107 6525 17001 1116 113 1386 114 117 1105 3937 3573 1107 1109 10763 19719 113 1386 114 1105 152 4832 24940 113 1371 114 119 1430 2742 1106 11333 5525 163 1105 14513 1104 142 5100 1600 1107 22393 2151 113 1369 114 4401 1123 1503 1312 117 146 7277 119 119 119 15327 17355 1200 2093 113 1369 114 117 1134 1486 1103 3485 1104 1123 13000 118 15550 15327 17355 1200 2093 1105 2829 170 1647 118 3545 1565 8645 2763 1107 1333 117 1259 3765 1104 1103 2381 1111 107 8353 10504 113 11913 170 8467 1113 1135 114 107 119 24041 1261 170 14938 1121 1390 1107 1333 1105 1261 1166 2635 1104 1123 1578 132 1123 2223 1312 125 113 1349 114 1108 2886 1143 24834 1200 1107 3586 117 12138 3095 19657 117 3011 3618 117 1105 3281 3960 119 1430 11774 10214 3049 2362 1312 117 24041 113 1381 114 117 1108 6019 1121 2166 6596 1118 1157 6700 1707 1105 10016 1104 9934 6621 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 84\n",
            "INFO:run_squad:end_position: 86\n",
            "INFO:run_squad:answer: Dream ##girl ##s\n",
            "INFO:run_squad:*** Example ***\n",
            "INFO:run_squad:unique_id: 1000000019\n",
            "INFO:run_squad:example_index: 19\n",
            "INFO:run_squad:doc_span_index: 0\n",
            "INFO:run_squad:tokens: [CLS] When did Bey ##on ##ce take a hiatus in her career and take control of her management ? [SEP] Following the di ##sband ##ment of Destiny ' s Child in June 2005 , she released her second solo album , B ' Day ( 2006 ) , which contained hits \" D ##é ##j ##à V ##u \" , \" I ##rre ##place ##able \" , and \" Beautiful Lia ##r \" . Beyoncé also ventured into acting , with a Golden Globe - nominated performance in Dream ##girl ##s ( 2006 ) , and starring roles in The Pink Panther ( 2006 ) and O ##bs ##essed ( 2009 ) . Her marriage to rapper Jay Z and portrayal of E ##tta James in Cadillac Records ( 2008 ) influenced her third album , I Am . . . Sasha Fi ##er ##ce ( 2008 ) , which saw the birth of her alter - ego Sasha Fi ##er ##ce and earned a record - setting six Grammy Awards in 2010 , including Song of the Year for \" Single Ladies ( Put a Ring on It ) \" . Beyoncé took a hiatus from music in 2010 and took over management of her career ; her fourth album 4 ( 2011 ) was subsequently me ##llow ##er in tone , exploring 1970s funk , 1980s pop , and 1990s soul . Her critically acclaimed fifth studio album , Beyoncé ( 2013 ) , was distinguished from previous releases by its experimental production and exploration of darker themes . [SEP]\n",
            "INFO:run_squad:token_to_orig_map: 20:0 21:1 22:2 23:2 24:2 25:3 26:4 27:4 28:4 29:5 30:6 31:7 32:8 33:8 34:9 35:10 36:11 37:12 38:13 39:14 40:14 41:15 42:15 43:15 44:16 45:16 46:16 47:16 48:17 49:18 50:19 51:20 52:20 53:20 54:20 55:20 56:21 57:21 58:21 59:21 60:22 61:22 62:22 63:22 64:22 65:22 66:22 67:23 68:24 69:24 70:25 71:25 72:25 73:25 74:26 75:27 76:28 77:29 78:30 79:30 80:31 81:32 82:33 83:34 84:34 85:34 86:35 87:36 88:37 89:37 90:37 91:38 92:38 93:38 94:38 95:39 96:40 97:41 98:42 99:43 100:44 101:45 102:46 103:46 104:46 105:47 106:48 107:48 108:48 109:49 110:49 111:49 112:49 113:50 114:51 115:52 116:53 117:54 118:55 119:56 120:57 121:58 122:59 123:59 124:60 125:61 126:62 127:63 128:64 129:64 130:64 131:65 132:66 133:67 134:68 135:68 136:69 137:70 138:70 139:70 140:70 141:71 142:72 143:72 144:72 145:73 146:73 147:73 148:73 149:74 150:75 151:76 152:77 153:78 154:79 155:80 156:80 157:80 158:81 159:82 160:82 161:82 162:83 163:84 164:85 165:86 166:86 167:86 168:87 169:88 170:89 171:90 172:91 173:91 174:92 175:93 176:94 177:95 178:96 179:97 180:98 181:98 182:99 183:100 184:100 185:101 186:102 187:103 188:104 189:104 190:104 191:104 192:105 193:106 194:107 195:108 196:109 197:110 198:111 199:112 200:113 201:114 202:115 203:116 204:117 205:118 206:119 207:119 208:120 209:121 210:122 211:123 212:124 213:124 214:124 215:125 216:126 217:127 218:127 219:127 220:128 221:129 222:129 223:130 224:131 225:132 226:132 227:133 228:134 229:134 230:135 231:136 232:137 233:137 234:138 235:139 236:140 237:141 238:142 239:143 240:143 241:144 242:145 243:145 244:145 245:145 246:146 247:147 248:148 249:149 250:150 251:151 252:152 253:153 254:154 255:155 256:156 257:157 258:158 259:159 260:159\n",
            "INFO:run_squad:token_is_max_context: 20:True 21:True 22:True 23:True 24:True 25:True 26:True 27:True 28:True 29:True 30:True 31:True 32:True 33:True 34:True 35:True 36:True 37:True 38:True 39:True 40:True 41:True 42:True 43:True 44:True 45:True 46:True 47:True 48:True 49:True 50:True 51:True 52:True 53:True 54:True 55:True 56:True 57:True 58:True 59:True 60:True 61:True 62:True 63:True 64:True 65:True 66:True 67:True 68:True 69:True 70:True 71:True 72:True 73:True 74:True 75:True 76:True 77:True 78:True 79:True 80:True 81:True 82:True 83:True 84:True 85:True 86:True 87:True 88:True 89:True 90:True 91:True 92:True 93:True 94:True 95:True 96:True 97:True 98:True 99:True 100:True 101:True 102:True 103:True 104:True 105:True 106:True 107:True 108:True 109:True 110:True 111:True 112:True 113:True 114:True 115:True 116:True 117:True 118:True 119:True 120:True 121:True 122:True 123:True 124:True 125:True 126:True 127:True 128:True 129:True 130:True 131:True 132:True 133:True 134:True 135:True 136:True 137:True 138:True 139:True 140:True 141:True 142:True 143:True 144:True 145:True 146:True 147:True 148:True 149:True 150:True 151:True 152:True 153:True 154:True 155:True 156:True 157:True 158:True 159:True 160:True 161:True 162:True 163:True 164:True 165:True 166:True 167:True 168:True 169:True 170:True 171:True 172:True 173:True 174:True 175:True 176:True 177:True 178:True 179:True 180:True 181:True 182:True 183:True 184:True 185:True 186:True 187:True 188:True 189:True 190:True 191:True 192:True 193:True 194:True 195:True 196:True 197:True 198:True 199:True 200:True 201:True 202:True 203:True 204:True 205:True 206:True 207:True 208:True 209:True 210:True 211:True 212:True 213:True 214:True 215:True 216:True 217:True 218:True 219:True 220:True 221:True 222:True 223:True 224:True 225:True 226:True 227:True 228:True 229:True 230:True 231:True 232:True 233:True 234:True 235:True 236:True 237:True 238:True 239:True 240:True 241:True 242:True 243:True 244:True 245:True 246:True 247:True 248:True 249:True 250:True 251:True 252:True 253:True 254:True 255:True 256:True 257:True 258:True 259:True 260:True\n",
            "INFO:run_squad:input_ids: 101 1332 1225 24896 1320 2093 1321 170 14938 1107 1123 1578 1105 1321 1654 1104 1123 2635 136 102 2485 1103 4267 22309 1880 1104 16784 112 188 6405 1107 1340 1478 117 1131 1308 1123 1248 3444 1312 117 139 112 2295 113 1386 114 117 1134 4049 4919 107 141 2744 3361 9183 159 1358 107 117 107 146 11604 11256 1895 107 117 1105 107 9896 25760 1197 107 119 24041 1145 22542 1154 3176 117 1114 170 3684 10421 118 3639 2099 1107 6525 17001 1116 113 1386 114 117 1105 3937 3573 1107 1109 10763 19719 113 1386 114 1105 152 4832 24940 113 1371 114 119 1430 2742 1106 11333 5525 163 1105 14513 1104 142 5100 1600 1107 22393 2151 113 1369 114 4401 1123 1503 1312 117 146 7277 119 119 119 15327 17355 1200 2093 113 1369 114 117 1134 1486 1103 3485 1104 1123 13000 118 15550 15327 17355 1200 2093 1105 2829 170 1647 118 3545 1565 8645 2763 1107 1333 117 1259 3765 1104 1103 2381 1111 107 8353 10504 113 11913 170 8467 1113 1135 114 107 119 24041 1261 170 14938 1121 1390 1107 1333 1105 1261 1166 2635 1104 1123 1578 132 1123 2223 1312 125 113 1349 114 1108 2886 1143 24834 1200 1107 3586 117 12138 3095 19657 117 3011 3618 117 1105 3281 3960 119 1430 11774 10214 3049 2362 1312 117 24041 113 1381 114 117 1108 6019 1121 2166 6596 1118 1157 6700 1707 1105 10016 1104 9934 6621 119 102 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:input_mask: 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:segment_ids: 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
            "INFO:run_squad:start_position: 172\n",
            "INFO:run_squad:end_position: 172\n",
            "INFO:run_squad:answer: 2010\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zB1pgrpx8B8t",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.train()\n",
        "for _ in trange(int(num_train_epochs), desc=\"Epoch\"):\n",
        "    for step, batch in enumerate(tqdm(train_dataloader, desc=\"Iteration\")):\n",
        "        batch = tuple(t.to(device) for t in batch)\n",
        "        input_ids, input_mask, segment_ids, start_positions, end_positions = batch\n",
        "        loss = model(input_ids, segment_ids, input_mask, start_positions, end_positions)\n",
        "        if gradient_accumulation_steps > 1:\n",
        "            loss = loss / gradient_accumulation_steps\n",
        "        loss.backward()\n",
        "        if (step + 1) % gradient_accumulation_steps == 0:\n",
        "            optimizer.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "\n",
        "# Save a trained model, configuration and tokenizer\n",
        "model_to_save = model.module if hasattr(model, 'module') else model  # Only save the model it-self\n",
        "# If we save using the predefined names, we can load using `from_pretrained`\n",
        "output_model_file = os.path.join(output_dir, WEIGHTS_NAME)\n",
        "output_config_file = os.path.join(output_dir, CONFIG_NAME)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}